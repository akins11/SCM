[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is dedicated to simplifying Supply Chain Management (SCM) using data related tools, models, and techniques. As someone who is still learning about this field, i am excited to share my journey and insight with you.\nOne of my main goals for this blog is to provide a platform for knowledge sharing using tools such as R, Python, SQL, etc. I believe that these tools can help us better understand and optimize supply chain processes, ultimately leading to increase efficiency and cost savings.\nI will explore various topics related to SCM, such as inventory optimization, logistics management and demand forecasting. I will also share my experiences with using different data analysis techniques, including data visualization and machine learning.\nWhile i am still learning, i hope that my blog can serve as a resource for others who are interested in exploring this fascinating field."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supply Chain Management Analysis",
    "section": "",
    "text": "Demystifying Database Normalization: Moving Single CSV Files into a Structured Database\n\n\n\n\n\n\n\ncode\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nAyomide Akinwande\n\n\n\n\n\n\n  \n\n\n\n\nProduct Reorder Point Calculation In Python\n\n\n\n\n\n\n\ncode\n\n\ninventory analysis\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nAyomide Akinwande\n\n\n\n\n\n\n  \n\n\n\n\nProduct Delivery Analysis\n\n\n\n\n\n\n\nR\n\n\ndelivery analysis\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nAyomide Akinwande\n\n\n\n\n\n\n  \n\n\n\n\nABC-XYZ Inventory Classification\n\n\n\n\n\n\n\ncode\n\n\ninventory analysis\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nAyomide Akinwande\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/abc-xyz-analysis/index.html",
    "href": "posts/abc-xyz-analysis/index.html",
    "title": "ABC-XYZ Inventory Classification",
    "section": "",
    "text": "Effective inventory management is essential in maintaining optimal levels of inventory which ensures that the right items are always available when needed while reducing the cost of holding inventory and ultimately avoiding stockouts or excess inventory.\nInventory classification using the ABC-XYZ analysis is a widely used technique which categorizes items based on their sales volume and demand variability, allowing businesses to have better control over thier inventories and to also use optimal strategies on items that are most important to their business. We will utilize the powerful framework of ABC-XYZ inventory classification model to analyze transaction data for products. By performing this analysis, we will be able to extract valuable insights and identify trends that can help inform strategic decisions around inventory management."
  },
  {
    "objectID": "posts/abc-xyz-analysis/index.html#abc-classification",
    "href": "posts/abc-xyz-analysis/index.html#abc-classification",
    "title": "ABC-XYZ Inventory Classification",
    "section": "ABC Classification",
    "text": "ABC Classification\n\ndiscuss\n\n\nprint(f\"Number of unique products (SKU) is {df_m['sku'].nunique()}\")\n\nNumber of unique products (SKU) is 5241\n\n\n\nabc_df = (\n    df_m\n        .groupby(\"sku\")\n        .agg(unique_purchases = (\"sku\", \"nunique\"),\n             total_demand = (\"quantity\", \"sum\"), \n             total_revenue = (\"sales_amount\", \"sum\"))\n        .sort_values(by=\"total_revenue\", ascending=False)\n        .reset_index()\n)\n\nabc_df.head()\n\n\n\n\n\n  \n    \n      \n      sku\n      unique_purchases\n      total_demand\n      total_revenue\n    \n  \n  \n    \n      0\n      3YDVF\n      1\n      926.0\n      29419.35\n    \n    \n      1\n      LJ26I\n      1\n      632.0\n      13571.45\n    \n    \n      2\n      IRQZ4\n      1\n      321.0\n      11681.63\n    \n    \n      3\n      DWJC4\n      1\n      823.0\n      10592.09\n    \n    \n      4\n      44RUE\n      1\n      218.0\n      8942.63\n    \n  \n\n\n\n\n\ndef abc_classify_product(cum_percent):\n    \"\"\"\n    Apply an ABC classification to each product based on \n    its ranked percentage revenue contribution. Any split \n    can be used to suit your data. \n\n    :param cum_percent: Running percentage of revenue contributed\n    :return: ABC inventory classification\n    \"\"\"\n\n    if cum_percent <= 80:\n        return 'A'\n    elif cum_percent > 80 and cum_percent <= 90:\n        return 'B'\n    else:\n        return 'C'\n\n\nabc_df = abc_df.assign(\n    # the cummlative sum of revenue genrated by each unique SKU.\n    revenue_cumsum = lambda d: d[\"total_revenue\"].cumsum(),\n    # percentage of revenue cummulative sum.\n    revenue_running_percent = lambda d: (d[\"revenue_cumsum\"] / d[\"total_revenue\"].sum()) * 100,\n    # assigning product class based on the pareto ...\n    abc_class = lambda d: d[\"revenue_running_percent\"].apply(abc_classify_product),\n    # simple rank of each revenue percentage.\n    abc_rank  = lambda d: d[\"revenue_running_percent\"].rank().astype(int)\n)\n\nabc_df.head()\n\n\n\n\n\n  \n    \n      \n      sku\n      unique_purchases\n      total_demand\n      total_revenue\n      revenue_cumsum\n      revenue_running_percent\n      abc_class\n      abc_rank\n    \n  \n  \n    \n      0\n      3YDVF\n      1\n      926.0\n      29419.35\n      29419.35\n      1.866325\n      A\n      1\n    \n    \n      1\n      LJ26I\n      1\n      632.0\n      13571.45\n      42990.80\n      2.727279\n      A\n      2\n    \n    \n      2\n      IRQZ4\n      1\n      321.0\n      11681.63\n      54672.43\n      3.468347\n      A\n      3\n    \n    \n      3\n      DWJC4\n      1\n      823.0\n      10592.09\n      65264.52\n      4.140295\n      A\n      4\n    \n    \n      4\n      44RUE\n      1\n      218.0\n      8942.63\n      74207.15\n      4.707603\n      A\n      5\n    \n  \n\n\n\n\n\ntemp = (\n    abc_df\n        .groupby(\"abc_class\")\n        .agg(total_products = ('sku', 'nunique'),\n             total_demand = ('total_demand', \"sum\"),\n             total_revenue = ('total_revenue', \"sum\"))\n        .reset_index()\n)\n\n\nNumber of Products\n\n(\n    g.ggplot(data=temp, mapping=g.aes(x=\"abc_class\", y=\"total_products\")) +\n    g.geom_col() +\n    g.labs(x=\"Class\", y=\"No. Products\", title=\"Number of Products in Each Class\") +\n    g.theme(figure_size=(10, 4))\n)\n\n\n\n\n<ggplot: (126679290061)>\n\n\n\n\nProduct Demand\n\n(\n    g.ggplot(data=temp, mapping=g.aes(x=\"abc_class\", y=\"total_demand\")) +\n    g.geom_col() +\n    g.labs(x=\"Class\", y=\"Qty. Demand\", title=\"Total Quantity Demand by (ABC) Class\") +\n    g.theme(figure_size=(10, 4))\n)\n\n\n\n\n<ggplot: (126567596005)>\n\n\n\n\nRevenue\n\n(\n    g.ggplot(data=temp, mapping=g.aes(x=\"abc_class\", y=\"total_revenue\")) +\n    g.geom_col() +\n    g.labs(x=\"Class\", y=\"Revenue\", title=\"Total Revenu by (ABC) Class\") +\n    g.theme(figure_size=(10, 4))\n)\n\n\n\n\n<ggplot: (126567633641)>"
  },
  {
    "objectID": "posts/abc-xyz-analysis/index.html#xyz-classification",
    "href": "posts/abc-xyz-analysis/index.html#xyz-classification",
    "title": "ABC-XYZ Inventory Classification",
    "section": "XYZ Classification",
    "text": "XYZ Classification\nOne major aspect ABC analysis considers is the importance of items based on their demand and revenue leaving out other factors such as demand variability and sessionality. This gap can be solved using the XYZ inventory classification which categorize products according to their level of demand variability.\nThere are three categories in XYZ analysis which are:\n\nClass X: These are items with low demand variability, they are less challenging to manage because of their predictable demand patterns.\n\n\nClass Y: Items in this class have a moderate demand variability they are less easer to manage than items in class X but have better predictable patterns than items in class z.\n\n\nClass Z: There is a high demand variability in items that falls within this category, demand for product are less stable and predictable.\n\n\n(\n    g.ggplot(data=df_m.groupby(\"month\")[\"quantity\"].sum().reset_index(), \n             mapping=g.aes(x=\"factor(month)\", y=\"quantity\", group=1)) +\n    g.geom_line() +\n    g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.labs(x=\"Month\", y=\"Quantity\", title=\"Quantity Demand by Month\") +\n    g.theme(figure_size=(10, 4)) \n)\n\n\n\n\n<ggplot: (193133998062)>\n\n\nA rundown of the seasonality in the data shows that the total customer order by month have huge variablility across the year with its lowest point in August while its peaked in December. It is important to also note that order started to increase from Match to May after that a sudden drop in order from June to August began.\n\n(\n    g.ggplot(data=df_m.groupby(\"month\")[\"sales_amount\"].sum().reset_index(), \n             mapping=g.aes(x=\"factor(month)\", y=\"sales_amount\", group=1)) +\n    g.geom_line() +\n    g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.labs(x=\"Month\", y=\"Revenue\", title=\"Total Revenue in Each Month\") +\n    g.theme_minimal() +\n    g.theme(figure_size=(10, 4)) \n)\n\n\n\n\n<ggplot: (193134591637)>\n\n\nAlthough there are similar seasonal pattern between the total quantity order and revenue, the upword trend in revenue appear to have began in January all the way through to May, before the expected drop in revenue similar to the quantity order.\nTo draw out the XYZ class, we must first reshape the data where each row represent a single SKU and the 12 months spread across different columns with values of the total quantity demand for a product.\n\nxyz_df = (\n    df_m\n        .groupby([\"sku\", \"month\"])[\"quantity\"]\n        .sum()\n        .reset_index()\n        .pivot(index=\"sku\", columns=\"month\", values=\"quantity\")\n        .fillna(0)\n        .add_prefix(\"M\")\n        .reset_index()\n        .rename(index={\"month\": \"index\"})\n)\n\nxyz_df.columns.name = \"\"\n\nxyz_df.head()\n\n\n\n\n\n  \n    \n      \n      sku\n      M1\n      M2\n      M3\n      M4\n      M5\n      M6\n      M7\n      M8\n      M9\n      M10\n      M11\n      M12\n    \n  \n  \n    \n      0\n      00GVC\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      00OK1\n      0.0\n      0.0\n      1.0\n      7.0\n      2.0\n      1.0\n      2.0\n      3.0\n      0.0\n      0.0\n      0.0\n      2.0\n    \n    \n      2\n      0121I\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      5.0\n      3.0\n      1.0\n      2.0\n      4.0\n      3.0\n      2.0\n    \n    \n      3\n      01IEO\n      3.0\n      3.0\n      5.0\n      8.0\n      6.0\n      3.0\n      3.0\n      3.0\n      0.0\n      3.0\n      2.0\n      4.0\n    \n    \n      4\n      01IQT\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n  \n\n\n\n\n\nxyz_df = xyz_df.assign(\n    # calculate the standard deviation of all months total demand.\n    std_demand   = lambda d: d[[c for c in xyz_df.columns if \"M\" in c]].std(axis=1),\n    # calculate the total demand across all months.\n    total_demand = lambda d: d[[c for c in xyz_df.columns if \"M\" in c]].sum(axis=1),\n    # calculate the average demand for each SKU.\n    avg_demand   = lambda d: d[\"total_demand\"] / 12,\n    # coefficient of variation for each SKU\n    cov_demand   = lambda d: d[\"std_demand\"] / d[\"avg_demand\"]\n)\n\nxyz_df.head()\n\n\n\n\n\n  \n    \n      \n      sku\n      M1\n      M2\n      M3\n      M4\n      M5\n      M6\n      M7\n      M8\n      M9\n      M10\n      M11\n      M12\n      std_demand\n      total_demand\n      avg_demand\n      cov_demand\n    \n  \n  \n    \n      0\n      00GVC\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.389249\n      2.0\n      0.166667\n      2.335497\n    \n    \n      1\n      00OK1\n      0.0\n      0.0\n      1.0\n      7.0\n      2.0\n      1.0\n      2.0\n      3.0\n      0.0\n      0.0\n      0.0\n      2.0\n      2.022600\n      18.0\n      1.500000\n      1.348400\n    \n    \n      2\n      0121I\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      5.0\n      3.0\n      1.0\n      2.0\n      4.0\n      3.0\n      2.0\n      1.311372\n      31.0\n      2.583333\n      0.507628\n    \n    \n      3\n      01IEO\n      3.0\n      3.0\n      5.0\n      8.0\n      6.0\n      3.0\n      3.0\n      3.0\n      0.0\n      3.0\n      2.0\n      4.0\n      2.020726\n      43.0\n      3.583333\n      0.563924\n    \n    \n      4\n      01IQT\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.792961\n      7.0\n      0.583333\n      1.359363\n    \n  \n\n\n\n\nCoefficient of variation or CV is a statistical measure that express the variability of a set of data as a percentage of its mean.\n\nFormula: CV = (standard deviation / mean)\n\nTo measure each SKU variability, its CV will be used to determine the level of variation. when the value closer to zero, it indicate a small amount of variability, while value further from zero indecate a high amount of variability.\n\n(\n    xyz_df[\"cov_demand\"]\n        .agg([\"min\", \"mean\", \"max\"])\n        .reset_index()\n        .rename(columns={\"index\": \"Stats\", \"cov_demand\": \"Coefficient of Variation\"})\n)\n\n\n\n\n\n  \n    \n      \n      Stats\n      Coefficient of Variation\n    \n  \n  \n    \n      0\n      min\n      0.095766\n    \n    \n      1\n      mean\n      1.748931\n    \n    \n      2\n      max\n      3.464102\n    \n  \n\n\n\n\nThe average CV of 1.75 indicate that the level of variability can be considered high since it falls within the CV range of 1-2, also the Maximum CV of 3.46 indicate that there are lot of products with high demand variability.\n\n(\n    g.ggplot(data=xyz_df, mapping=g.aes(x=\"cov_demand\")) +\n    g.geom_histogram(color=\"#FFF\", bins = 20) +\n    g.ggtitle(\"Coefficient of Variation\") +\n    g.theme(figure_size=(10, 4))\n)\n\n\n\n\n<ggplot: (193134606240)>\n\n\n\nXYZ classes\nWe will use the 0.5/1/> 1 CV to group all product into XYZ classes. products with a CV less tha 0.5 indicate low demand variability and better forecating, this category of products will be assgined the X class while product with CV of 0.5 to 1 indicate a moderate demand variability and will make up the Y class. Products with CV greater than 1 are much harder to predict and are subject to high fluctuations throughout the year and they will be categorised as Z class.\n\ndef xyz_classifer(cov): # change name\n    \"\"\"\n    Assign (XYZ) categories to each SKU based on their coefficient of \n    variation (CV) in order quantity.\n\n    :param cov: Coefficient of variation in order quantity for SKU\n    :return:  A pandas Series with three unique category.\n    \"\"\"\n\n    if cov <= 0.5:\n        return 'X'\n    elif cov > 0.5 and cov <= 1.0:\n        return 'Y'\n    else:\n        return 'Z'\n\n\nxyz_df = xyz_df.assign(xyz_class = xyz_df[\"cov_demand\"].apply(xyz_classifer))\n\n(\n    xyz_df[\"xyz_class\"]\n        .value_counts()\n        .reset_index()\n        .rename(columns={\"index\": \"Class\", \"xyz_class\": \"count\"})\n)\n\n\n\n\n\n  \n    \n      \n      Class\n      count\n    \n  \n  \n    \n      0\n      Z\n      3429\n    \n    \n      1\n      Y\n      1281\n    \n    \n      2\n      X\n      531\n    \n  \n\n\n\n\nBased on the number of product in each class, there are more products in the Z class than other classes this is expected given the distribution analysis earlier.\n\n# Tabular Summary\n(\n    xyz_df\n        .groupby(\"xyz_class\")\n        .agg(total_product = (\"sku\", \"nunique\"),\n             total_demand = (\"total_demand\", \"sum\"),\n             std_of_demand = (\"std_demand\", \"mean\"),\n             average_demand = (\"avg_demand\", \"mean\"),\n             average_cov = (\"cov_demand\", \"mean\"))\n        .reset_index()\n)\n\n\n\n\n\n  \n    \n      \n      xyz_class\n      total_product\n      total_demand\n      std_of_demand\n      average_demand\n      average_cov\n    \n  \n  \n    \n      0\n      X\n      531\n      99436.733\n      4.981240\n      15.605263\n      0.369697\n    \n    \n      1\n      Y\n      1281\n      65623.300\n      3.037538\n      4.269015\n      0.736462\n    \n    \n      2\n      Z\n      3429\n      30348.396\n      1.210885\n      0.737542\n      2.340750\n    \n  \n\n\n\n\n\nmonth_cols = [c for c in xyz_df.columns if \"M\" in c]\n\ntemp = (\n    xyz_df\n        .groupby(\"xyz_class\")[month_cols]\n        .sum()\n        .unstack(level=\"xyz_class\")\n        .reset_index()\n        .rename(columns={0: \"total_demand\", \"\": \"month\"})\n        .assign(month=lambda d: d[\"month\"].astype(\"category\").cat.reorder_categories(month_cols, ordered=True))\n)\n\ntemp.head()\n\n\n\n\n\n  \n    \n      \n      month\n      xyz_class\n      total_demand\n    \n  \n  \n    \n      0\n      M1\n      X\n      7955.50\n    \n    \n      1\n      M1\n      Y\n      4737.00\n    \n    \n      2\n      M1\n      Z\n      2181.87\n    \n    \n      3\n      M2\n      X\n      7851.49\n    \n    \n      4\n      M2\n      Y\n      4794.34\n    \n  \n\n\n\n\n\n(\n    g.ggplot(data=temp, mapping=g.aes(x=\"month\", y=\"total_demand\", group=1)) +\n    g.geom_line() +\n    g.scale_y_continuous(labels= lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.facet_wrap(facets=\"xyz_class\", nrow=3, scales=\"free_y\") +\n    g.labs(x=None, y=\"Demand\", title=\"(XYZ) Class by Total Quantity Order in each Month\") +\n    custom_theme(figure_size=(10, 6))\n)\n\n\n\n\n<ggplot: (193134620162)>\n\n\nExamining each category represented on the graph, class X have the least amount of flutuations compared to class Y and class Z, class Z represent a clear case of high volatility on quantity demand across all months."
  },
  {
    "objectID": "posts/abc-xyz-analysis/index.html#abc-xyz-inventory-analysis",
    "href": "posts/abc-xyz-analysis/index.html#abc-xyz-inventory-analysis",
    "title": "ABC-XYZ Inventory Classification",
    "section": "ABC-XYZ inventory analysis",
    "text": "ABC-XYZ inventory analysis\nCombining both ABC and XYZ classes provides inventory managers with a clear insight on the most important items in the inventory, based on the amount of revenue they contribute and also their demand patterns. This information can help them determine the appropriate stock levels which reduces the risk of either stockouts or overstocking.\n\nMerge ABC and XYZ class tables\n\nabc_xyz = abc_df[[\"sku\", \"total_revenue\", \"abc_class\"]].merge(\n    xyz_df[[\"sku\", \"std_demand\", \"total_demand\", \"avg_demand\", \"cov_demand\", \"xyz_class\"]],\n    on=\"sku\",\n    how=\"left\"\n)\n\nabc_xyz.head()\n\n\n\n\n\n\n\n\nsku\ntotal_revenue\nabc_class\nstd_demand\ntotal_demand\navg_demand\ncov_demand\nxyz_class\n\n\n\n\n0\n3YDVF\n29419.35\nA\n7.529739\n926.0\n77.166667\n0.097578\nX\n\n\n1\nLJ26I\n13571.45\nA\n13.773052\n632.0\n52.666667\n0.261514\nX\n\n\n2\nIRQZ4\n11681.63\nA\n10.703653\n321.0\n26.750000\n0.400137\nX\n\n\n3\nDWJC4\n10592.09\nA\n8.273268\n823.0\n68.583333\n0.120631\nX\n\n\n4\n44RUE\n8942.63\nA\n15.602059\n218.0\n18.166667\n0.858829\nY\n\n\n\n\n\n\n\n\n\nCombining ABC & XYZ classes\n\nabc_xyz = abc_xyz.assign(abc_xyz_class = abc_xyz[\"abc_class\"].astype(str) + abc_xyz[\"xyz_class\"].astype(str))\n\nabc_xyz.head()\n\n\n\n\n\n\n\n\nsku\ntotal_revenue\nabc_class\nstd_demand\ntotal_demand\navg_demand\ncov_demand\nxyz_class\nabc_xyz_class\n\n\n\n\n0\n3YDVF\n29419.35\nA\n7.529739\n926.0\n77.166667\n0.097578\nX\nAX\n\n\n1\nLJ26I\n13571.45\nA\n13.773052\n632.0\n52.666667\n0.261514\nX\nAX\n\n\n2\nIRQZ4\n11681.63\nA\n10.703653\n321.0\n26.750000\n0.400137\nX\nAX\n\n\n3\nDWJC4\n10592.09\nA\n8.273268\n823.0\n68.583333\n0.120631\nX\nAX\n\n\n4\n44RUE\n8942.63\nA\n15.602059\n218.0\n18.166667\n0.858829\nY\nAY\n\n\n\n\n\n\n\nThis combination create 9 unique classes which are AX, AY, AZ, BX, BY, BZ, CX, CY, and CZ and each SKU falls under one of them. Each combined class can be summarised as follow:\nAX : This category consist of high revenue generating items with a stable demand which makes them easy to forecast.\nAY : Also a high revenue generating items but with less stable demand, as demand varies over time.\nAZ : They are tough to forecaset because of their irregular demand nature, but they are also a high revenue generating item.\nBX : They generate moderate revenue and have a stable demand across all month.\nBY : Items in this category are less stable based on demand but they generate a moderate amount of revenue.\nBZ : Revenue generated from items in this category are moderate but forecasting demand for them can be very challenging.\nCX : Items in this category are easy to forecast but they generate a very low amount of revenue.\nCY : This category includes items with less stable demand and also low revenue.\nCZ : The items here are difficult to forecast because demand may be periodic and they also generate a very small amount of revenue.\n\ntemp = (\n    abc_xyz\n        .groupby(\"abc_xyz_class\")\n        .agg(total_sku=('sku', 'nunique'),\n             total_demand = ('total_demand', \"sum\"),\n             avg_demand = ('avg_demand', 'mean'),    \n             total_revenue = ('total_revenue', \"sum\"))\n        .reset_index()\n)\n\ntemp\n\n\n\n\n\n\n\n\nabc_xyz_class\ntotal_sku\ntotal_demand\navg_demand\ntotal_revenue\n\n\n\n\n0\nAX\n412\n86198.533\n17.434978\n622917.97\n\n\n1\nAY\n519\n38869.630\n6.241110\n512348.97\n\n\n2\nAZ\n204\n5086.510\n2.077823\n125490.43\n\n\n3\nBX\n104\n12588.200\n10.086699\n21662.63\n\n\n4\nBY\n575\n22001.670\n3.188648\n103916.87\n\n\n5\nBZ\n731\n11243.900\n1.281794\n111158.97\n\n\n6\nCX\n15\n650.000\n3.611111\n896.88\n\n\n7\nCY\n187\n4752.000\n2.117647\n9454.98\n\n\n8\nCZ\n2494\n14017.986\n0.468390\n68477.72\n\n\n\n\n\n\n\n\n\nCode\ndef abc_xyz_plot(var: str, y_title: str, title: str):\n    return (\n        g.ggplot(data=temp, mapping=g.aes(x=\"abc_xyz_class\", y=var)) +\n        g.geom_col() +\n        g.labs(x=\"Class\", y=y_title, title=title) +\n        g.scale_y_continuous(labels=lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n        custom_theme()\n    )\n\n\n\nabc_xyz_plot(var=\"total_sku\", \n             y_title=\"No. Products\", \n             title=\"Number of Products by (ABC-XYZ) Class\")\n\n\n\n\n47.6% of all products fall under the CZ class, with this insight inventory managers need to develope a responsive or adaptive supply chain for these products that can instantly adjust to changes in demand, where inventory is oredered and received only when it is needed, which reduces carrying cost.\n\nabc_xyz_plot(var=\"total_demand\", \n             y_title=\"Demand\", \n             title=\"Total Quantity Demand by (ABC-XYZ) Class\")\n\n\n\n\nFor quantity demand, class AX holds the majority with 44% of all demand. Since only 7.86% of all product falls under the AX class, inventory managers can bring in more products from other class by reviewing price strategy, imporving marketing and promotion and also consider bundling or cross-selling with other high performing products such as products in class AX and AY.\n\nabc_xyz_plot(var=\"total_revenue\", \n             y_title=\"Revenue\", \n             title=\"Total Revenue by (ABC-XYZ) Class\")\n\n\n\n\nClass AX and AY both contains majority of products that generate the highest revenue, a combined 72% of revenue. Inventory managers should consider conducting market research on products generating less revenue to identify trends and customer preferences. This can help them identify new opputunities and adjust product features and marketing accordingly.\nIn summary ABC-XYZ analysis provides insight that inventory managers can utilize to optimize inventory business operations, by indentifying the most critical items and assigning appproprate inventory control policies, businesses can better manage their stock and ultimately improve customer satisfaction. Additionally, its can help inventory managers to identify opportunities for process improvement, enhance decision-making, and allocate resources effectively."
  },
  {
    "objectID": "posts/abc-xyz-analysis/index.html#abc-inventory-classification",
    "href": "posts/abc-xyz-analysis/index.html#abc-inventory-classification",
    "title": "ABC-XYZ Inventory Classification",
    "section": "ABC inventory classification",
    "text": "ABC inventory classification\nABC inventory classification helps business group items based on demand, cost, revenue and the overall relative importance of the item to the business. It helps them realize which of their products or services have the most significant impact on their financial success.\nIt categories items into three separate classes where:\n\nItems in class A are considared to have high demand volume and revenue, majorly 80% overall.\n\n\nItems in class B have slightly moderate demand volume and revenue, considared to be 15% of the overall total.\n\n\nItems in class C have reletively low demand volume and revenue.\n\nTo group each product, we will perform the following operations:\n\nGroup the transaction data by SKU and extract the number of times each product was purchased, the total demand for the product and the sum of all sales amount.\nThe resulting data from the previous step will be arranged from the highest revenue to the lowest revenue (e.i in descending order).\n\n\nabc_df = (\n    df_m\n        .groupby(\"sku\")\n        .agg(unique_purchases = (\"sku\", \"nunique\"),\n             total_demand = (\"quantity\", \"sum\"), \n             total_revenue = (\"sales_amount\", \"sum\"))\n        .sort_values(by=\"total_revenue\", ascending=False)\n        .reset_index()\n)\n\nabc_df.head()\n\n\n\n\n\n\n\n\nsku\nunique_purchases\ntotal_demand\ntotal_revenue\n\n\n\n\n0\n3YDVF\n1\n926.0\n29419.35\n\n\n1\nLJ26I\n1\n632.0\n13571.45\n\n\n2\nIRQZ4\n1\n321.0\n11681.63\n\n\n3\nDWJC4\n1\n823.0\n10592.09\n\n\n4\n44RUE\n1\n218.0\n8942.63\n\n\n\n\n\n\n\n\nGet the cumulative sum of total revenue which determine the relative contribution of each product to the total revenue generated from all products.\nProducts that belong to the A class are those that accounted for 80% of the revenue generated.\nClass B products are those that fall between the top 80% and the next 15% of revenue generated.\nWhile products in class C are those that fall withing the remaining 5% of revenue generated.\n\n\n\nCode\ndef classify_abc_item(cum_percent):\n    \"\"\"\n    Assign (ABC) classification to each product based on the their percentage of\n    revenue contribution. \n\n    :param cum_percent: The running percentage of revenue contributed.\n    :return: A pandas Series with three unique category.\n    \"\"\"\n\n    if cum_percent &lt;= 80:\n        return 'A'\n    elif cum_percent &gt; 80 and cum_percent &lt;= 95:\n        return 'B'\n    else:\n        return 'C'\n\n\n\nabc_df = abc_df.assign(\n    # running total of revenue.\n    revenue_cumsum = lambda d: d[\"total_revenue\"].cumsum(),\n    # the percentage of total revenue.\n    revenue_running_percent = lambda d: (d[\"revenue_cumsum\"] / d[\"total_revenue\"].sum()) * 100,\n    # aasign each product a class.\n    abc_class = lambda d: d[\"revenue_running_percent\"].apply(classify_abc_item),\n    # simple rank to each product based on the its percentage of revenue generated.\n    rank  = lambda d: d[\"revenue_running_percent\"].rank().astype(int)\n)\n\nabc_df.head()\n\n\n\n\n\n\n\n\nsku\nunique_purchases\ntotal_demand\ntotal_revenue\nrevenue_cumsum\nrevenue_running_percent\nabc_class\nrank\n\n\n\n\n0\n3YDVF\n1\n926.0\n29419.35\n29419.35\n1.866325\nA\n1\n\n\n1\nLJ26I\n1\n632.0\n13571.45\n42990.80\n2.727279\nA\n2\n\n\n2\nIRQZ4\n1\n321.0\n11681.63\n54672.43\n3.468347\nA\n3\n\n\n3\nDWJC4\n1\n823.0\n10592.09\n65264.52\n4.140295\nA\n4\n\n\n4\n44RUE\n1\n218.0\n8942.63\n74207.15\n4.707603\nA\n5\n\n\n\n\n\n\n\n\n\nSummary\n\ntemp = (\n    abc_df\n        .groupby(\"abc_class\")\n        .agg(total_products = ('sku', 'nunique'),\n             total_demand = ('total_demand', \"sum\"),\n             total_revenue = ('total_revenue', \"sum\"))\n        .reset_index()\n)\n\n\n\nCode\ndef abc_plot(var: str, y_title: str, title: str):\n    return (\n        g.ggplot(data=temp.assign(prop = lambda d: (d[var] / d[var].sum())*100), \n             mapping=g.aes(x=\"abc_class\", y=var)) +\n        g.geom_col() +\n        g.geom_text(g.aes(label=\"prop\", y=0), \n                    position=g.position_dodge(width=.9), \n                    nudge_x=-0.18,\n                    size=30, color=\"#4D4D4D\",\n                    va=\"bottom\",\n                    format_string=\"{:.1f}%\") +\n        g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n        g.labs(x=\"Class\", y=y_title, title=title) +\n        custom_theme()\n    )\n\n\n\nNumber of Products\n\nabc_plot(var=\"total_products\", \n         y_title=\"No. Products\", \n         title=\"Number of Products in Each Class\")\n\n\n\n\nA large number of products, e.i about 51.4% fall within the class C boundary which have the lowest demand and revenue generated while 21.7% of all products falls within the top 80% generating the highest revenue.\n\n\nProduct Demand\n\nabc_plot(var=\"total_demand\", \n         y_title=\"Qty. Demand\", \n         title=\"Total Quantity Demand by (ABC) Class\")\n\n\n\n\nProduct classification summary by the quantity show how much of products in class A where in demand. There are no suprises here given that these products generate more revenue. For class C products, despite comprising of 51.4% of all products, they only have 9.9% of the total order quantity.\n\n\nRevenue\n\n(\n    g.ggplot(data=temp, mapping=g.aes(x=\"abc_class\", y=\"total_revenue\")) +\n    g.geom_col() +\n    g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.labs(x=\"Class\", y=\"Revenue\", title=\"Total Revenue by (ABC) Class\") +\n    custom_theme()\n)\n\n\n\n\nA visual representation of the 80/15/5 split of the total revenue generated and as expected all products generating more demand and revenue are in the class A category. It is important for inventory managers to create an optimize inventory level for such product to meet demand, they should also prioritize supplier relationship to ensure adequate access to such products, and lastly there should be a huge focus on quality as anything less than that can lead to decrease in demand and eventually low revenue."
  },
  {
    "objectID": "posts/abc-xyz-analysis/index.html#xyz-inventory-classification",
    "href": "posts/abc-xyz-analysis/index.html#xyz-inventory-classification",
    "title": "ABC-XYZ Inventory Classification",
    "section": "XYZ inventory classification",
    "text": "XYZ inventory classification\nThe major aspect of ABC analysis is it focus on valuable items based on their demand and revenue with less emphasis on other factors such as demand variability and sessionality. This gap can be bridged using the XYZ inventory classification which categorize products according to their level of demand variability.\nThere are three categories in XYZ analysis which are:\n\nClass X: These are items with low demand variability, they are less challenging to manage because of their predictable demand patterns.\n\n\nClass Y: Items in this class have a moderate demand variability, but they are not easy to manage unlike items in class X, but they have better predictable patterns than items in class Z.\n\n\nClass Z: There are high demand variability in items that falls within this category, and demand for product are less stable and predictable.\n\n\n(\n    g.ggplot(data=df_m.groupby(\"month\")[\"quantity\"].sum().reset_index(), \n             mapping=g.aes(x=\"factor(month)\", y=\"quantity\", group=1)) +\n    g.geom_line() +\n    g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.labs(x=\"Month\", y=\"Quantity\", title=\"Quantity Demand by Month\") +\n    custom_theme()\n)\n\n\n\n\nA rundown of the seasonality in the data indicate that the total customer order by month have huge variablility across the year with its lowest point in August while its peaked in December. It is crucial to note that there was a surge in order from March to May followed by an abrupt decline in order from June to August.\n\n(\n    g.ggplot(data=df_m.groupby(\"month\")[\"sales_amount\"].sum().reset_index(), \n             mapping=g.aes(x=\"factor(month)\", y=\"sales_amount\", group=1)) +\n    g.geom_line() +\n    g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.labs(x=\"Month\", y=\"Revenue\", title=\"Total Revenue in Each Month\") +\n    custom_theme() \n)\n\n\n\n\nAlthough there are similar seasonal pattern between the total quantity order and revenue, the upword trend in revenue appear to have began in January all the way through to May, before the expected drop in revenue similar to the quantity order.\nTo draw out the XYZ class, we must first reshape the data and make each row represent a single SKU and the 12 months spread across different columns with values of the total quantity demand for a particular SKU.\n\nxyz_df = (\n    df_m\n        .groupby([\"sku\", \"month\"])[\"quantity\"]\n        .sum()\n        .reset_index()\n        .pivot(index=\"sku\", columns=\"month\", values=\"quantity\")\n        .fillna(0)\n        .add_prefix(\"M\")\n        .reset_index()\n        .rename(index={\"month\": \"index\"})\n)\n\nxyz_df.columns.name = \"\"\n\nxyz_df.head()\n\n\n\n\n\n\n\n\nsku\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nM8\nM9\nM10\nM11\nM12\n\n\n\n\n0\n00GVC\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n00OK1\n0.0\n0.0\n1.0\n7.0\n2.0\n1.0\n2.0\n3.0\n0.0\n0.0\n0.0\n2.0\n\n\n2\n0121I\n1.0\n3.0\n4.0\n2.0\n1.0\n5.0\n3.0\n1.0\n2.0\n4.0\n3.0\n2.0\n\n\n3\n01IEO\n3.0\n3.0\n5.0\n8.0\n6.0\n3.0\n3.0\n3.0\n0.0\n3.0\n2.0\n4.0\n\n\n4\n01IQT\n0.0\n2.0\n0.0\n0.0\n0.0\n1.0\n1.0\n2.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\nxyz_df = xyz_df.assign(\n    # calculate the standard deviation of all months total demand.\n    std_demand   = lambda d: d[[c for c in xyz_df.columns if \"M\" in c]].std(axis=1),\n    # calculate the total demand across all months.\n    total_demand = lambda d: d[[c for c in xyz_df.columns if \"M\" in c]].sum(axis=1),\n    # calculate the average demand for each SKU.\n    avg_demand   = lambda d: d[\"total_demand\"] / 12,\n    # coefficient of variation for each SKU\n    cov_demand   = lambda d: d[\"std_demand\"] / d[\"avg_demand\"]\n)\n\nxyz_df.head()\n\n\n\n\n\n\n\n\nsku\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nM8\nM9\nM10\nM11\nM12\nstd_demand\ntotal_demand\navg_demand\ncov_demand\n\n\n\n\n0\n00GVC\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.389249\n2.0\n0.166667\n2.335497\n\n\n1\n00OK1\n0.0\n0.0\n1.0\n7.0\n2.0\n1.0\n2.0\n3.0\n0.0\n0.0\n0.0\n2.0\n2.022600\n18.0\n1.500000\n1.348400\n\n\n2\n0121I\n1.0\n3.0\n4.0\n2.0\n1.0\n5.0\n3.0\n1.0\n2.0\n4.0\n3.0\n2.0\n1.311372\n31.0\n2.583333\n0.507628\n\n\n3\n01IEO\n3.0\n3.0\n5.0\n8.0\n6.0\n3.0\n3.0\n3.0\n0.0\n3.0\n2.0\n4.0\n2.020726\n43.0\n3.583333\n0.563924\n\n\n4\n01IQT\n0.0\n2.0\n0.0\n0.0\n0.0\n1.0\n1.0\n2.0\n0.0\n0.0\n0.0\n1.0\n0.792961\n7.0\n0.583333\n1.359363\n\n\n\n\n\n\n\nCoefficient of variation : or CV is a statistical measure that express the variability of a set of data as a percentage of its mean.\n\nFormula: CV = (standard deviation / mean)\n\nTo measure each SKU variability, its CV will be used to determine the level of variation. So values are closer to zero, it indicate a small amount of variability, while value further from zero indicate a high amount of variability.\n\n(\n    xyz_df[\"cov_demand\"]\n        .agg([\"min\", \"mean\", \"max\"])\n        .reset_index()\n        .rename(columns={\"index\": \"Stats\", \"cov_demand\": \"Coefficient of Variation\"})\n)\n\n\n\n\n\n\n\n\nStats\nCoefficient of Variation\n\n\n\n\n0\nmin\n0.095766\n\n\n1\nmean\n1.748931\n\n\n2\nmax\n3.464102\n\n\n\n\n\n\n\nThe average CV of 1.75 indicate that the level of variability in demand is high since it falls within the CV range of 1-2, also the Maximum CV of 3.46 indicate that there are lot of products with high demand variability.\n\n(\n    g.ggplot(data=xyz_df, mapping=g.aes(x=\"cov_demand\")) +\n    g.geom_histogram(color=\"#FFF\", bins = 20) +\n    g.ggtitle(\"Coefficient of Variation\") +\n    custom_theme()\n)\n\n\n\n\n\nXYZ classes\nWe will use the 0.5 / 1 / &gt;1 CV to group all product into XYZ classes. products with a CV less than 0.5 indicate low demand variability and better forecating, this category of products will be assgined the X class while product with CV that falls within 0.5 to 1 indicate a moderate demand variability and will make up the Y class. Products with CV greater than 1 are much harder to predict and are subject to high fluctuations throughout the year and they will be categorised as Z class.\n\n\nCode\ndef classify_xyz_item(cov): \n    \"\"\"\n    Assign (XYZ) categories to each SKU based on their coefficient of \n    variation (CV) in order quantity.\n\n    :param cov: Coefficient of variation in order quantity for SKU\n    :return:  A pandas Series with three unique category.\n    \"\"\"\n\n    if cov &lt;= 0.5:\n        return 'X'\n    elif cov &gt; 0.5 and cov &lt;= 1.0:\n        return 'Y'\n    else:\n        return 'Z'\n\n\n\nxyz_df = xyz_df.assign(xyz_class = xyz_df[\"cov_demand\"].apply(classify_xyz_item))\n\n(\n    xyz_df[\"xyz_class\"]\n        .value_counts()\n        .reset_index()\n        .rename(columns={\"index\": \"Class\", \"xyz_class\": \"count\"})\n)\n\n\n\n\n\n\n\n\nClass\ncount\n\n\n\n\n0\nZ\n3429\n\n\n1\nY\n1281\n\n\n2\nX\n531\n\n\n\n\n\n\n\nBased on the number of product in each class, there are more products in the Z class than other classes, this is expected given the CV distribution earlier.\n\n(\n    xyz_df\n        .groupby(\"xyz_class\")\n        .agg(total_product = (\"sku\", \"nunique\"),\n             total_demand = (\"total_demand\", \"sum\"),\n             std_of_demand = (\"std_demand\", \"mean\"),\n             average_demand = (\"avg_demand\", \"mean\"),\n             average_cov = (\"cov_demand\", \"mean\"))\n        .reset_index()\n)\n\n\n\n\n\n\n\n\nxyz_class\ntotal_product\ntotal_demand\nstd_of_demand\naverage_demand\naverage_cov\n\n\n\n\n0\nX\n531\n99436.733\n4.981240\n15.605263\n0.369697\n\n\n1\nY\n1281\n65623.300\n3.037538\n4.269015\n0.736462\n\n\n2\nZ\n3429\n30348.396\n1.210885\n0.737542\n2.340750\n\n\n\n\n\n\n\n\nmonth_cols = [c for c in xyz_df.columns if \"M\" in c]\n\ntemp = (\n    xyz_df\n        .groupby(\"xyz_class\")[month_cols]\n        .sum()\n        .unstack(level=\"xyz_class\")\n        .reset_index()\n        .rename(columns={0: \"total_demand\", \"\": \"month\"})\n        .assign(month=lambda d: d[\"month\"].astype(\"category\").cat.reorder_categories(month_cols, ordered=True))\n)\n\ntemp.head()\n\n\n\n\n\n\n\n\nmonth\nxyz_class\ntotal_demand\n\n\n\n\n0\nM1\nX\n7955.50\n\n\n1\nM1\nY\n4737.00\n\n\n2\nM1\nZ\n2181.87\n\n\n3\nM2\nX\n7851.49\n\n\n4\nM2\nY\n4794.34\n\n\n\n\n\n\n\n\n(\n    g.ggplot(data=temp, mapping=g.aes(x=\"month\", y=\"total_demand\", group=1)) +\n    g.geom_line() +\n    g.scale_y_continuous(labels= lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n    g.facet_wrap(facets=\"xyz_class\", nrow=3, scales=\"free_y\") +\n    g.labs(x=None, y=\"Demand\", title=\"(XYZ) Class by Total Quantity Order in each Month\") +\n    custom_theme(figure_size=(10, 6))\n)\n\n\n\n\nExamining each category represented on the graph, class X have the least amount of flutuations compared to class Y and class Z, class Z represent a clear case of high volatility on quantity demand across all months."
  },
  {
    "objectID": "posts/Delivery-analysis/index.html",
    "href": "posts/Delivery-analysis/index.html",
    "title": "Product Delivery Analysis",
    "section": "",
    "text": "This report present an analysis of the delivery performance within an online marketplace for a specific period in 2018, spanning from January to August. The analysis will primarily focus on the various stages involved in the delivery process, specifically examining the duration between the time an order was placed, its approval, the transition to the carrier, and ultimately reaching the final customers.\nIt is important to note that the analysis encompasses a duration of only 8 months in 2018, excluding the incomplete data for the month of September and October. Due to certain data limitations, the analysis for September and October lacks complete information for all its days. However, the report aims to provide valuable data and [drawing meaningful conclusion for the preceding months].\nUnderstanding the efficiency and effectiveness of the delivery process is crucial for any online marketplace. Customerâ€™s satisfaction and loyalty heavily rely on the timely and reliable delivery of their orders. Therefore, this report aims to shed light on the delivery performance, offering key observations, trends, and potential areas for improvement. Without further ado, let us delve into the delivery analysis."
  },
  {
    "objectID": "posts/Delivery-analysis/index.html#introduction",
    "href": "posts/Delivery-analysis/index.html#introduction",
    "title": "Product Delivery Analysis",
    "section": "",
    "text": "This report present an analysis of the delivery performance within an online marketplace for a specific period in 2018, spanning from January to August. The analysis will primarily focus on the various stages involved in the delivery process, specifically examining the duration between the time an order was placed, its approval, the transition to the carrier, and ultimately reaching the final customers.\nIt is important to note that the analysis encompasses a duration of only 8 months in 2018, excluding the incomplete data for the month of September and October. Due to certain data limitations, the analysis for September and October lacks complete information for all its days. However, the report aims to provide valuable data and [drawing meaningful conclusion for the preceding months].\nUnderstanding the efficiency and effectiveness of the delivery process is crucial for any online marketplace. Customerâ€™s satisfaction and loyalty heavily rely on the timely and reliable delivery of their orders. Therefore, this report aims to shed light on the delivery performance, offering key observations, trends, and potential areas for improvement. Without further ado, let us delve into the delivery analysis."
  },
  {
    "objectID": "posts/Delivery-analysis/index.html#analysis",
    "href": "posts/Delivery-analysis/index.html#analysis",
    "title": "Product Delivery Analysis",
    "section": "Analysis",
    "text": "Analysis\n\n\nShow the Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(gt)\nlibrary(gtExtras)\n\n\n\n\nShow the Code\ncsv_name &lt;- c(\n  \"order_items\", \n  \"orders\",\n  \"product_category_name\",\n  \"products\",\n  \"sellers\"\n)\n\nall_datasets &lt;- map(\n  csv_name, \\(table) read_csv(glue::glue(\"Data/olist_{table}_dataset.csv\"))\n)\nnames(all_datasets) &lt;- csv_name\n\norders &lt;- all_datasets$orders\norder_items &lt;- all_datasets$order_items\nproducts &lt;- all_datasets$products\nproduct_category_name &lt;- all_datasets$product_category_name\nsellers &lt;- all_datasets$sellers\n\nrm(all_datasets)\n\nbz_state_name &lt;- read_csv(\"Data/brazil-state-name.csv\")\n\n\n\n\nShow the Code\norders &lt;- orders |&gt;\n  filter(year(order_purchase_timestamp) == 2018,\n         !month(order_purchase_timestamp) %in% c(9, 10),\n         !month(order_delivered_customer_date) %in% c(9, 10))\n\n\n\n\nShow the Code\nperiod_levels &lt;- c(\"minutes\", \"hours\", \"days\", \"weeks\")\n\ncolr &lt;- list(\n  title = \"#343A40\", \n  line = \"#495057\",\n  ax_title = \"#6C757D\",\n  bar = \"#4A4E69\",\n  legend = \"#46494C\"\n)\n\ncustom_theme &lt;- function(...) {\n    ggplot2::theme_minimal() +\n    ggplot2::theme(plot.title = element_text(colour = colr$title),\n                   axis.title = element_text(color = colr$ax_title),\n                   legend.title = element_text(color = colr$legend),\n                   legend.text = element_text(color = colr$legend),\n                   ...)\n}\n\n\n\nOrder volume by time period\n\nNumber of orders delivered to customer across 2018:\n\n\nShow the Code\ntemp &lt;- orders |&gt;\n  filter(!is.na(order_delivered_customer_date)) |&gt;\n  select(order_id, order_delivered_customer_date) |&gt;\n  mutate(day = day(order_delivered_customer_date),\n         week_day = wday(order_delivered_customer_date, label = TRUE),\n         month = month(order_delivered_customer_date, label = TRUE),\n         day_year = yday(order_delivered_customer_date))\n\n\ntemp |&gt; \n  group_by(date = date(order_delivered_customer_date)) |&gt;\n  summarise(count = n()) |&gt;\n  \n  ggplot(aes(x = date, y = count, group = 1)) +\n  geom_line(color = colr$line) +\n  labs(x = NULL, y = \"Count\", title = \"Number of Orders Supplied to Customers\") +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  custom_theme()\n\n\n\n\n\nThe trend plot analysis reveals significant variability in the number of deliveries made each day throughout the selected period. This indicate that there were fluctuations in the order volume with some days witnessing a high number of deliveries while others had comparatively lower activity. This variability also suggests the influence of various factors such as customer demand, marketing campaigns, seasonal trends, or external events that impacted the order on a daily basis. The analysis also identifies specific dates that stand out in terms of number of orders delivered. The largest delivery day occurred on August 27th, on the other hand the analysis highlight two days, namely February 12th and July 22nd, with the lowest number of orders supplied. Interestingly, these days recorded only a single order being fulfilled. The low activity in these days could be an indication of various factors such as low customer demand, seasonality or other external factors affecting order placement.\n\n\n\nNumber of orders delivered on weekdays and weekends:\n\n\nShow the Code\ntemp |&gt; \n  summarise(count = n(), .by = week_day) |&gt; \n  mutate(percentage = proportions(count)*100) |&gt; \n  \n  ggplot(aes(x = week_day, y = count)) +\n  geom_col(fill = colr$bar) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(x = NULL, y = \"Count\", title = \"Number of Supplied Order by Day of the Week\") +\n  custom_theme()\n\n\n\n\n\nThe above plot indicates that a majority of the orders supplied to customers, specifically 92.7%, took place on weekdays, while the remaining 7.3% occurred on weekends. This distribution indicate more customers received their orders during weekdays compared to weekends. Analyzing the distribution of delivery within weekdays, the plot reveals interesting patterns. Among all the weekdays, Mondays stand as the day with the highest number of orders received by customers, accounting for 19% of the total. On the other hand, the analysis highlights that Sundays had the lowest number of orders received, with only a handful of orders (593) being fulfilled. suggest This indicates relatively lower customer engagement on weekends especially on Sundays. These insights can be used to align staffing levels, logistics, and customer support accordingly, ensuring timely and reliable deliveries while meeting customer expectations\n\n\n\nNumber of orders delivered in each month:\n\n\nShow the Code\ntemp |&gt;\n  summarise(count = n(), .by = month) |&gt;\n  mutate(percentage = proportions(count)*100) |&gt; \n  \n  ggplot(aes(x = month, y = count, group = 1)) +\n  geom_line(color = colr$line, linewidth = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(x = NULL, y = \"Count\", title = \"Number of Supplied Order by Month\") +\n  custom_theme()\n\n\n\n\n\nThe analysis of the number of orders received by month reveals several key trends. In the first quarter of 2018, there was a steady increase in the number of deliveries, indicating a positive growth trajectory. This upward trend continued until April, which emerged as the second-best month in terms of the number of orders delivered. However, the second quarter witnessed a decline in delivery volume. This decline persisted throughout the quarter, with a notable dip in July. Interestingly, there was a significant upturn in delivery performance during August. This month witnessed a drastic increase in the number of orders delivered, signaling a substantial improvement compared to the preceding months.The analysis indicates that August accounted for approximately 15.8% of the total orders delivered during the year, making it the month with the most product deliveries. This high delivery volume suggests increased customer activity, potentially due to various factors such as seasonal sales, promotional campaigns, or other market influences driving demand.\nUnderstanding the dynamics of August and the factors contributing to its exceptional delivery performance can provide valuable insight. These insights can inform future strategies, such as capitalizing on peak demand periods, optimizing logistics operations, and planning marketing initiatives to replicate or even surpass the success achieved during August.\n \n\n\n\nOrder status\n\n\nShow the Code\norders |&gt;\n  count(order_status, sort = TRUE, name = \"count\") |&gt;\n  mutate(percentage = round(proportions(count)*100, 2),\n         order_status = str_to_title(order_status)) |&gt;\n  \n  gt() |&gt;\n  cols_label(order_status = \"Status\",\n             count = md(\"*No.* Orders\"),\n             percentage = \"Percentage\") |&gt; \n  tab_header(\"Order Volume by Order Status\") |&gt;\n  fmt_integer(columns = count) |&gt;\n  tab_style(style = cell_text(\"#52B788\", weight = \"bold\"), \n            locations = cells_body(order_status, 1)) |&gt;\n  tab_style(style = cell_text(\"#FFC300\", weight = \"bold\"), \n            locations = cells_body(order_status, 2)) |&gt;\n  tab_style(style = cell_text(\"#D90429\", weight = \"bold\"), \n            locations = cells_body(order_status, c(3, 4)))\n\n\n\n\n\n\n  \n    \n      Order Volume by Order Status\n    \n    \n    \n      Status\n      No. Orders\n      Percentage\n    \n  \n  \n    Delivered\n52,724\n97.76\n    Shipped\n567\n1.05\n    Canceled\n315\n0.58\n    Unavailable\n145\n0.27\n    Invoiced\n121\n0.22\n    Processing\n59\n0.11\n    Created\n1\n0.00\n  \n  \n  \n\n\n\n\nThe analysis indicates that a significant majority, specifically 97.8%, of the orders where delivered successfully. Successful delivery implies that the orders were processed, shipped, and received by the customers. This high percentage reflects the effectiveness and efficiency of the delivery process.\nA small percentage, approximately 1.05%, of the orders (567 orders) were still in transit at the time of the analysis. These orders indicate that they have been shipped or dispatched but have not yet reached the final customers.\nThe analysis also reveals that a portion of the orders experienced cancellation or unavailability. Approximately 0.58% of all orders (315 orders) were canceled, indicating that these orders were requested to be canceled by either the customers or the seller. The reasons for cancellation can vary, including customer changes of mind, product unavailability, or other logistical or operational factors.\n\n\nOrder status and order month\n\n\nShow the Code\nOSL &lt;- c(\"delivered\", \"shipped\", \"invoiced\", \"processing\", \"canceled\", \"unavailable\")\n\norders |&gt;\n  mutate(order_month = month(order_purchase_timestamp, label = TRUE)) |&gt;\n  group_by(order_month, order_status) |&gt;\n  summarise(count = n()) |&gt;\n  mutate(total_order = sum(count),\n         order_status = factor(order_status, levels = OSL),\n         order_status = str_to_title(order_status)) |&gt;\n  filter(order_status != \"created\") |&gt;\n  \n  \n  ggplot(aes(x = order_month, y = count)) +\n  geom_line(aes(group = 1), color = colr$line, linewidth = 0.7) +\n  facet_wrap(vars(order_status), ncol = 2, scales = \"free_y\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(x = NULL, y = \"Count\", title = \"Order Status and Purchase Month\") +\n  custom_theme()\n\n\n\n\n\nThe analysis of the order status across all available months indicates that there was a steady decline in the number of orders shipped after March, with a few exceptions. Specifically, the number of orders shipped decreased steadily from April to June. However, in July, there was an increase of 3% in the total number of shipped orders. The highest cancellation occurred in August which saw a 51% increase in canceled orders. Both processing and delivered order kept declining after March. **********\n \n\n\n\nDuration between purchase date and order approval\n\n\nShow the Code\nclean_duration &lt;- function(string) {\n  stringr::str_extract(string, \"\\\\((.*?)\\\\)\") |&gt; \n    stringr::str_replace(\"~\", \"\") |&gt; \n    stringr::str_replace(\"\\\\(\", \"\") |&gt;\n    stringr::str_replace(\"\\\\)\", \"\")\n}\n\ntemp &lt;- orders |&gt;\n  select(order_purchase_timestamp, order_approved_at) |&gt;\n  mutate(approval_diff = as.numeric(order_approved_at - order_purchase_timestamp),\n         duration = dminutes(x = approval_diff) |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"d_value\", \"period\"), sep = \" \", remove = FALSE) |&gt;\n  mutate(d_value = as.numeric(d_value),\n         period = factor(period, levels = period_levels))\n\n\n\nTime summary:\n\n\nShow the Code\ntemp |&gt;\n  count(period, sort = FALSE, name = \"count\") |&gt;\n  mutate(percentage = round(proportions(count)*100, 2),\n         period = str_to_title(period),\n         count_label = count) |&gt;\n  \n  gt() |&gt;\n  cols_label(period = md(\"*Unite* of Time\"),\n             count = md(\"*No.* of Order\"),\n             percentage = \"Percentage\",\n             count_label = \" \") |&gt;\n  fmt_integer(columns = count) |&gt;\n  sub_missing() |&gt;\n  tab_style(style = cell_text(weight = \"bold\", color = \"#6C757D\"),\n            locations = cells_body(columns = period)) |&gt;\n  tab_style(cell_text(size = \"small\", weight = \"lighter\"),\n            cells_body(rows = 5)) |&gt;\n  gt_plt_bar(column = count_label, color = colr$bar)\n\n\n\n\n\n\n  \n    \n    \n      Unite of Time\n      No. of Order\n      Percentage\n       \n    \n  \n  \n    Minutes\n34,144\n63.31\n          \n    Hours\n10,628\n19.71\n          \n    Days\n9,068\n16.81\n          \n    Weeks\n29\n0.05\n          \n    â€”\n63\n0.12\n          \n  \n  \n  \n\n\n\n\nA significant majority, approximately 63.31%, of the orders were approved within minutes of order initiation. This high percentage suggests an efficient system in place to review and approve orders swiftly, enhancing customer satisfaction and reducing potential delays. On the other end of the spectrum, a small percentage of orders (0.05%) took up to a week to be approved. This indicates that a very limited number of orders experienced delays in the approval process. The analysis highlights that there are more orders approved within hours (19.7%) compared to those that take days (16.8%).\nThe presence of orders that take days for approval suggests that certain factors, such as order complexity, stock availability, or manual review processes, may contribute to the longer approval time for a subset of orders\n\n\n\nMonth summary:\n\n\nShow the Code\norders |&gt;\n  filter(!is.na(order_approved_at)) |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE)) |&gt;\n  summarise(count = n(), .by = month) |&gt;\n  \n  ggplot(aes(x = month, y = count)) +\n  geom_col(fill = colr$bar) +\n  geom_text(aes(label = scales::comma(count), y = 300), \n            hjust = 0.5, \n            color = \"#ADB5BD\",\n            size = 5) +\n  labs(x = NULL, y = NULL, title = \"Number of Transactions by Month\") +\n  custom_theme(axis.text.y = element_blank())\n\n\n\n\n\n\n\nShow the Code\ntemp |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE)) |&gt;\n  group_by(month) |&gt;\n  summarise(n_order = n(),\n            median = median(approval_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = dminutes(x = median) |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"d_value\", \"period\"), sep = \" \", remove = FALSE) |&gt;\n  filter(!is.na(median)) |&gt;\n  \n  ggplot(aes(x = month, y = d_value)) +\n  geom_col(fill = colr$bar) +\n  labs(x = NULL, \n       y = \"Minutes\", \n       title = \"Median Duration to Approved Orders\",\n       fill = \"Period\") +\n  custom_theme()\n\n\n\n\n\nJuly stands out as the month with the highest median approval duration, approximately 32.2 minutes. This indicates that, on average, it took slightly longer for orders to be approved in July compared to other months. Also August emerges as the month with the best order approval duration, with an approximate median of 16.83 minutes which indicates that, on average, orders were approved relatively quickly. Interestingly, despite experiencing higher transaction volumes in January and March, these months had median approval duration of 19.2 and 20.6 minutes, respectively. This indicates that even though more orders were being processed during these months, the median approval duration on average where faster than other months except for August.\nBy identifying the factors contributing to longer approval duration and leveraging best practices from months with shorter duration, approval process can be optimize to ensure timely order processing for improved customer satisfaction.\n \n\n\n\nDuration up to carrier\n\n\nShow the Code\ntemp &lt;- orders |&gt;\n  select(order_purchase_timestamp, order_approved_at, order_delivered_carrier_date) |&gt;\n  mutate(\n    pur_carr_diff = as.numeric(order_delivered_carrier_date - order_purchase_timestamp),\n    pc_duration = dseconds(x = pur_carr_diff) |&gt; clean_duration(),\n    appr_carr_diff = as.numeric(order_delivered_carrier_date - order_approved_at),\n    ac_duration = dseconds(x = appr_carr_diff) |&gt; clean_duration(),\n  ) |&gt;\n  separate(col = pc_duration, into = c(\"pc_value\", \"pc_period\"), sep = \" \") |&gt;\n  separate(col = ac_duration, into = c(\"ac_value\", \"ac_period\"), sep = \" \") |&gt;\n  mutate(across(c(pc_value, ac_value), as.numeric),\n         across(c(pc_period, ac_period), \\(col) factor(col, levels = period_levels)))\n\n\n\nOrder to Carrier:\n\n\nShow the Code\ntemp |&gt; \n  count(pc_period, name = \"no_order\") |&gt;\n  mutate(percentage = proportions(no_order)*100, \n         pc_period = str_to_title(pc_period),\n         no_order_bar = no_order) |&gt;\n  \n  gt() |&gt;\n  cols_label(pc_period = md(\"*Unite* of Time\"),\n             no_order = md(\"*No.* of Order\"),\n             percentage = \"Percentage\",\n             no_order_bar = \" \") |&gt;\n  fmt_number(columns = percentage) |&gt;\n  fmt_integer(columns = no_order) |&gt;\n  sub_missing() |&gt;\n  tab_style(style = cell_text(weight = \"bold\", color = \"#6C757D\"),\n            locations = cells_body(columns = pc_period)) |&gt;\n  tab_style(cell_text(size = \"small\", weight = \"lighter\"),\n            cells_body(rows = 5)) |&gt;\n  gt_plt_bar(column = no_order_bar, color = colr$bar)\n\n\n\n\n\n\n  \n    \n    \n      Unite of Time\n      No. of Order\n      Percentage\n       \n    \n  \n  \n    Minutes\n290\n0.54\n          \n    Hours\n11,402\n21.14\n          \n    Days\n37,826\n70.14\n          \n    Weeks\n3,834\n7.11\n          \n    â€”\n580\n1.08\n          \n  \n  \n  \n\n\n\n\nThe analysis indicates that majority of orders, approximately 70%, were delivered to the carrier within days which suggests that the majority of orders required more time for approval preparation, packaging, and coordination before they could be handed over to the carrier for transportation. Approximately 21.1% of the orders were delivered to the carrier within hours. This subset of orders represents a relatively faster delivery process, indicating that a portion of the orders was quickly approved, prepared and made ready for shipping. While a small percentage, approximately 0.5%, of the orders were delivered to the carrier within minute\n\n\nDelivery to carrier summary:\n\n\nShow the Code\ntemp |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE)) |&gt;\n  \n  group_by(month) |&gt;\n  summarise(no_order = n(),\n            pc_avg = mean(pur_carr_diff, na.rm = TRUE),\n            pc_median = median(pur_carr_diff, na.rm = TRUE)) |&gt;\n  mutate(pc_duration = dseconds(x = pc_median) |&gt; clean_duration()) |&gt;\n  separate(col = pc_duration, into = c(\"pc_value\", \"pc_period\"), sep = \" \") |&gt;\n  filter(!is.na(pc_value)) |&gt;\n  \n  ggplot(aes(x = month, y = pc_value, group = 1)) +\n  geom_line(color = colr$line, linewidth = 0.7) +\n  labs(x = NULL, y = \"Days\", title = \"Median Duration of Product Delivered to Carrier\") +\n  custom_theme()\n\n\n\n\n\nThe analysis reveals a consistent decline in the median number of days for orders to move from sellers to carriers across all months. The decreasing median duration indicates a positive trend toward faster order handover and readiness for transportation. It also suggests an improvement in the efficiency of this phase of the delivery process over time. February stands out as the month with the highest median duration for orders to reach carriers from sellers, with a value of 2.52 days. While August recorded the lowest median duration.\n\n\n\n\nFrom approval date to carrier:\n\n\nShow the Code\ntemp |&gt;\n  select(order_purchase_timestamp, ac_value, ac_period) |&gt;\n  mutate(app_point = if_else(ac_value &lt;= 0, \"Late\", \"Early\")) |&gt;\n  count(app_point, sort = TRUE, name = \"count\") |&gt;\n  mutate(percentage = proportions(count)*100,\n         count_label = count) |&gt;\n  \n  gt() |&gt;\n  cols_label(app_point = \"Approval Term\",\n             count = md(\"*No.* Order\"),\n             percentage = \"Percentage\",\n             count_label = \" \") |&gt;\n  fmt_number(columns = percentage) |&gt;\n  fmt_integer(columns = count) |&gt;\n  sub_missing() |&gt;\n  tab_style(style = cell_text(weight = \"bold\", color = \"#6C757D\"),\n            locations = cells_body(columns = app_point)) |&gt;\n  tab_style(cell_text(size = \"small\", weight = \"lighter\"),\n            cells_body(rows = 3)) |&gt;\n  gt_plt_bar(column = count_label, color = colr$bar)\n\n\n\n\n\n\n  \n    \n    \n      Approval Term\n      No. Order\n      Percentage\n       \n    \n  \n  \n    Early\n52,084\n96.57\n          \n    Late\n1,272\n2.36\n          \n    â€”\n576\n1.07\n          \n  \n  \n  \n\n\n\n\n\n\nShow the Code\ntemp |&gt;\n  mutate(Approval = if_else(ac_value &lt;= 0, \"After\", \"Before\")) |&gt;\n  summarise(n_order = n(), .by = c(ac_period, Approval)) |&gt;\n  mutate(percentage = proportions(n_order)*100,\n         n_order_label = n_order,\n         ac_period = str_to_title(ac_period)) |&gt;\n  filter(!is.na(ac_period)) |&gt;\n  group_by(Approval) |&gt;\n  \n  gt() |&gt;\n  cols_label(ac_period = md(\"*Unite* of Time\"),\n             n_order = md(\"*No.* of Order\"),\n             percentage = \"Percentage\",\n             n_order_label = \" \") |&gt;\n  fmt_number(percentage) |&gt;\n  fmt_integer(n_order) |&gt;\n  gt_plt_bar(column = n_order_label, color = colr$bar) |&gt;\n  tab_options(column_labels.font.size = 16,\n              row_group.background.color = \"#FAFAFA\",\n              row_group.font.size = 13)\n\n\n\n\n\n\n  \n    \n    \n      Unite of Time\n      No. of Order\n      Percentage\n       \n    \n  \n  \n    \n      Before\n    \n    Hours\n16,416\n30.44\n          \n    Days\n32,169\n59.65\n          \n    Weeks\n3,201\n5.94\n          \n    Minutes\n298\n0.55\n          \n    \n      After\n    \n    Days\n425\n0.79\n          \n    Hours\n615\n1.14\n          \n    Minutes\n231\n0.43\n          \n    Weeks\n1\n0.00\n          \n  \n  \n  \n\n\n\n\nThere were instances where some orders were delivered to the carrier before they were officially approved. This may be a potential breakdown in the order processing system, where orders may have been dispatched to the carrier prematurely, leading to a mismatch between the approval status and the actual delivery process. Looking at the numbers 1,274 orders, accounting for approximately 2.4% of the total, were approved after they had already been delivered to the carrier. Identifying the reasons behind this delay and implementing measures to streamline the approval process can help avoid such instances in the future.\nLooking at the duration, 59.7% of orders took at least a day from the time of approval to the time they were delivered to the carriers. Additionally, 30.4% of orders had duration in hours from approval to delivery.\n\n\n\nShow the Code\ntemp |&gt;\n  select(order_purchase_timestamp, appr_carr_diff, ac_value, ac_period) |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE),\n         approval = if_else(ac_value &lt;= 0, \"After Approval\", \"Before Approval\")) |&gt;\n  filter(!is.na(approval)) |&gt;\n\n  summarise(avg = mean(appr_carr_diff, na.rm = TRUE),\n            median = median(appr_carr_diff, na.rm = TRUE),\n            .by = c(month, approval)) |&gt;\n  mutate(duration = dseconds(x = median) |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = str_replace(value, \"-\", \"\")) |&gt;\n  \n  ggplot(aes(x = month, y = value, fill = period)) +\n  geom_col() +\n  facet_wrap(vars(factor(approval, levels = c(\"Before Approval\", \"After Approval\"))), \n             ncol = 1, \n             scales = \"free_y\") +\n  labs(x = NULL, y = \"Duration\", title = \"Duration of Orders From Approval to Carrier\") +\n  scale_fill_manual(values = c(\"#BC4B51\", \"#5B8E7D\", \"#52B788\"), name = \"Period\") +\n  custom_theme() \n\n\n\n\n\nThere was a positive decline in the duration it took for products to be delivered to the carrier after approval. This decline started from February and continued through to June. Factors contributing to this decline could include streamlined workflows, optimized logistics, enhanced coordination with carriers, and improved operational practices.\n \n\n\n\nDuration of delivery to customers.\n\n\nShow the Code\ntemp &lt;- orders |&gt; \n  select(order_purchase_timestamp, \n         order_delivered_carrier_date, \n         order_delivered_customer_date) |&gt; \n  mutate(\n    pur_cus_diff = as.numeric(order_delivered_customer_date - order_purchase_timestamp),\n    pc_duration = duration(pur_cus_diff, \"hours\") |&gt; clean_duration(),\n    carr_cus_diff = as.numeric(order_delivered_customer_date - order_delivered_carrier_date),\n    cc_duration = dseconds(x = carr_cus_diff) |&gt; clean_duration(),\n  ) |&gt;\n\n  separate(col = pc_duration, into = c(\"pc_value\", \"pc_period\"), sep = \" \") |&gt;\n  separate(col = cc_duration, into = c(\"cc_value\", \"cc_period\"), sep = \" \") |&gt;\n  mutate(across(c(pc_value, cc_value), as.numeric),\n         across(c(pc_period, cc_period), \\(col) factor(col, levels = period_levels))) \n\n\n\nDelivery duration to customer\n\n\nShow the Code\ntemp |&gt;\n  count(pc_period, name = \"n_orders\") |&gt;\n  mutate(percentage = proportions(n_orders)*100,\n         pc_period = str_to_title(pc_period),\n         n_order_label = n_orders) |&gt;\n  \n  gt() |&gt;\n  cols_label(pc_period = md(\"*Unit* of Time\"),\n             n_orders = md(\"*No.* of Order\"),\n             percentage = \"Percentage\",\n             n_order_label = \" \") |&gt;\n  tab_header(\"Duration of Delivery to Customers\") |&gt;\n  fmt_number(percentage) |&gt;\n  fmt_integer(n_orders) |&gt;\n  sub_missing() |&gt;\n  tab_style(cell_text(size = \"small\", weight = \"lighter\"),\n            cells_body(rows = 4)) |&gt;\n  gt_plt_bar(column = n_order_label, color = colr$bar)\n\n\n\n\n\n\n  \n    \n      Duration of Delivery to Customers\n    \n    \n    \n      Unit of Time\n      No. of Order\n      Percentage\n       \n    \n  \n  \n    Hours\n6\n0.01\n          \n    Days\n16,577\n30.74\n          \n    Weeks\n36,136\n67.00\n          \n    â€”\n1,213\n2.25\n          \n  \n  \n  \n\n\n\n\n\n\nShow the Code\ntemp |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE)) |&gt;\n  group_by(month) |&gt;\n  summarise(avg = mean(pur_cus_diff, na.rm = TRUE),\n            median = median(pur_cus_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = duration(median, \"hours\") |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = as.numeric(value)) |&gt;\n  filter(!is.na(median)) |&gt;\n  \n  ggplot(aes(x = month, y = value, group = 1)) +\n  geom_line(color = colr$line) +\n  labs(x = NULL, y = \"Weeks\", title = \"Median Duration of Receiving an Order\") +\n  scale_y_continuous(breaks = seq(0, 3.0, 0.2)) +\n  custom_theme()\n\n\n\n\n\nThe analysis reveals a positive decline in the duration of order delivery to customers over each passing month. Given that approximately 67% of customers received their orders in at least a week, February had the highest median duration for order delivery, taking over two weeks. On the other hand, August had the lowest median duration for order delivery, taking approximately one week. This follows the same declining trend of the median delivery duration to carrier except for July when there was an increase.\n\n\n\nShow the Code\ntemp |&gt;\n  mutate(day = day(order_purchase_timestamp)) |&gt; \n  group_by(day) |&gt;\n  summarise(avg = mean(pur_cus_diff, na.rm = TRUE),\n            median = median(pur_cus_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = duration(median, \"hours\") |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = as.numeric(value)) |&gt;\n  filter(!is.na(median)) |&gt;\n\n  ggplot(aes(x = day, y = value, group = 1)) +\n  geom_line(color = colr$line) +\n  labs(x = \"Day of the month\", \n       y = \"Weeks\", \n       title = \"Median Duration of Receiving an Order\") +\n  scale_x_continuous(breaks = seq(1, 31, 2)) +\n  custom_theme()\n\n\n\n\n\nCertain days, such as the 25th, 15th, 18th, 21st, and 7th, recorded the lowest delivery duration. These days experienced shorter delivery times on average compared to other days of the month. On the other hand, specific days, such as the 10th, 12th, and 27th, recorded the highest delivery duration.\n\n\n\ncarrier to customer:\n\n\nShow the Code\ntemp |&gt;\n  count(cc_period, name = \"n_orders\") |&gt;\n  mutate(percentage = proportions(n_orders)*100,\n         cc_period = str_to_title(cc_period),\n         n_order_label = n_orders) |&gt;\n  \n  gt() |&gt;\n  cols_label(cc_period = md(\"*Unit* of Time\"),\n             n_orders = md(\"*No.* of Order\"),\n             percentage = \"Percentage\",\n             n_order_label = \" \") |&gt;\n  tab_header(\"Duration of Delivery from Carrier to Customer\") |&gt;\n  fmt_number(percentage) |&gt;\n  fmt_integer(n_orders) |&gt;\n  sub_missing() |&gt;\n  tab_style(cell_text(size = \"small\", weight = \"lighter\"),\n            cells_body(rows = 4)) |&gt;\n  gt_plt_bar(column = n_order_label, color = colr$bar)\n\n\n\n\n\n\n  \n    \n      Duration of Delivery from Carrier to Customer\n    \n    \n    \n      Unit of Time\n      No. of Order\n      Percentage\n       \n    \n  \n  \n    Hours\n1,313\n2.43\n          \n    Days\n25,795\n47.83\n          \n    Weeks\n25,607\n47.48\n          \n    â€”\n1,217\n2.26\n          \n  \n  \n  \n\n\n\n\n\n\nShow the Code\ntemp |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE)) |&gt;\n  group_by(month) |&gt;\n  summarise(avg = mean(carr_cus_diff, na.rm = TRUE),\n            median = median(carr_cus_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = duration(median, \"seconds\") |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  # mutate(value = as.numeric(value)) |&gt;\n  filter(!is.na(median))  |&gt;\n  \n  ggplot(aes(x = month, y = value, fill = period)) +\n  geom_col() +\n  labs(x = NULL, \n       y = \"Duration\", \n       title = \"Median Duration of Receiving an Order After Approval\") +\n  scale_fill_manual(values = c(\"#5B8E7D\", \"#BC4B51\"), name = \"Unite of Time\") +\n  custom_theme()\n\n\n\n\n\nMajority of orders were delivered to customers after some days and weeks. Furthermore, a small percentage (approximately 2.4%) of orders were delivered to customers within hours. The analysis highlights that the first three months had median delivery duration of over a week. This aligns with the previous analysis, which identified February as having the highest median duration for order delivery. In contrast to the longer August had the lowest overall duration for order delivery, with just 5 days\n \n\n\n\nEstimated order delivery date vs actual delivery date\n\n\nShow the Code\ntemp &lt;- orders |&gt;\n  select(order_purchase_timestamp,\n         order_delivered_customer_date, \n         order_estimated_delivery_date) |&gt;\n  mutate(\n    actual_diff = as.numeric(order_delivered_customer_date - order_purchase_timestamp),\n    a_duration = duration(actual_diff, \"hours\") |&gt; clean_duration(),\n    estimate_diff = as.numeric(order_estimated_delivery_date - order_purchase_timestamp),\n    e_duration = duration(estimate_diff, \"days\") |&gt; clean_duration(),\n  ) |&gt;\n  separate(col = a_duration, into = c(\"a_value\", \"a_period\"), sep = \" \") |&gt;\n  separate(col = e_duration, into = c(\"e_value\", \"e_period\"), sep = \" \") |&gt;\n  mutate(across(c(a_value, e_value), as.numeric),\n         across(c(a_period, e_period), \\(col) factor(col, levels = period_levels))) \n\n\n\nEstimated date\n\n\nShow the Code\ntemp |&gt;\n  count(e_period, name = \"n_order\") |&gt;\n  mutate(percentage = proportions(n_order)*100,\n         e_period = str_to_title(e_period),\n         n_order_label = n_order) |&gt;\n  \n  gt() |&gt;\n  cols_label(e_period = md(\"*Unit* of Time\"),\n             n_order = md(\"*No. Order*\"),\n             percentage = \"Percentage\",\n             n_order_label = \" \") |&gt;\n  tab_header(\"Duration of Delivery Based on Estimated Date\") |&gt;\n  fmt_number(percentage) |&gt;\n  fmt_integer(n_order) |&gt;\n  gt_plt_bar(column = n_order_label, color = colr$bar)\n\n\n\n\n\n\n  \n    \n      Duration of Delivery Based on Estimated Date\n    \n    \n    \n      Unit of Time\n      No. Order\n      Percentage\n       \n    \n  \n  \n    Days\n1,300\n2.41\n          \n    Weeks\n52,632\n97.59\n          \n  \n  \n  \n\n\n\n\n\n\nShow the Code\ntemp |&gt;\n  mutate(month = month(order_purchase_timestamp, label = TRUE),\n         day = day(order_purchase_timestamp)) |&gt;\n  group_by(month) |&gt;\n  summarise(n_order = n(),\n            median_actual = median(actual_diff, na.rm = TRUE),\n            median_estimate = median(estimate_diff, na.rm = TRUE)) |&gt;\n  mutate(actual_duration = duration(median_actual, \"hours\") |&gt; clean_duration(),\n         estimate_duration = duration(median_estimate, \"days\") |&gt; clean_duration()) |&gt;\n  separate(col = actual_duration, into = c(\"Actual\", \"a_period\"), sep = \" \") |&gt;\n  separate(col = estimate_duration, into = c(\"Estimated\", \"e_period\"), sep = \" \") |&gt;\n  \n  pivot_longer(cols = c(Actual, Estimated), names_to = \"ae\", values_to = \"duration\") |&gt;\n  filter(!is.na(duration)) |&gt;\n  \n  ggplot(aes(x = month, y = duration, group = ae, color = ae)) +\n  geom_line() +\n  scale_color_manual(values = c(\"#118AB2\", \"#06D6A0\"), name = \" \") +\n  labs(x = NULL, \n       y = \"Duration (Weeks)\", \n       title = \"Median Duration of Receiving an Order Based on the Actual & Estimate\") +\n  custom_theme()\n\n\n\n\n\nApproximately 97.6% of orders were estimated to take at least a week before being delivered to the customer. Offering a consistent estimated delivery time helps manage customer expectations and allows them to plan accordingly. However, when comparing the estimated duration to the actual delivery time, it is observed that the only (67%) of orders were delivered in a week or more, while a significant portion (30.74%) were delivered in a day or more.\nThe estimated delivery duration was generally longer than the actual delivery duration across all available months The average estimated duration was approximately 3 weeks, while the average actual duration was approximately 1 weeks. This indicates that the platform tended to provide customers with a more conservative estimated time frame for order delivery.\nThe analysis also identifies a significant decline in the actual delivery duration from February (2 weeks) to August (1 week) as against the estimated duration of which there was an expected increase from march (3 weeks) to June (4 weeks).\nBy further refining the estimation processes and closely monitoring actual delivery times, the online marketplace can align estimated and actual delivery duration more closely, ensuring accurate expectations and improved customer satisfaction.\n \n\n\n\nProduct specifications and delivary time\n\n\nShow the Code\ntemp &lt;-  orders |&gt;\n  left_join(select(order_items, order_id, product_id), by = \"order_id\") |&gt;\n  left_join(products, by = \"product_id\") |&gt;\n  select(-c(product_category_name, product_name_lenght, product_description_lenght)) |&gt;\n  select(order_purchase_timestamp, \n         order_delivered_customer_date,\n         product_weight_g, product_length_cm, product_height_cm, product_width_cm) |&gt; \n  mutate(\n    time_diff = as.numeric(order_delivered_customer_date - order_purchase_timestamp),\n    duration = duration(time_diff, \"hours\") |&gt; clean_duration()\n  ) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = as.numeric(value),\n         period = factor(period, levels = period_levels))\n\n\n\n\nShow the Code\ntemp |&gt; \n  select(product_weight_g, time_diff, value, period) |&gt;\n  filter(!is.na(time_diff)) |&gt;\n  \n  ggplot(aes(x = product_weight_g, y = time_diff, color = period)) +\n  geom_point() +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  scale_color_manual(values = c(\"#52B788\", \"#5B8E7D\", \"#BC4B51\"), name = \" \") +\n  labs(x = \"Product Weight (grams)\",\n       y = \"Time Difference\",\n       title = \"Relationship between Product Weight and Order Duration\") +\n  custom_theme()\n\n\n\n\n\n\n\nShow the Code\ntemp |&gt;\n  filter(!is.na(time_diff)) %&gt;% \n  cor.test(~product_weight_g + time_diff, data = .)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  product_weight_g and time_diff\nt = 23.82, df = 60255, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.08867028 0.10449025\nsample estimates:\n       cor \n0.09658636 \n\n\nThere is very low correlation coefficient of 0.09658636 between product weight and order duration. A correlation coefficient close to zero suggests that there is almost no linear relationship between these variables. In other words, the weight of the product does not have a significant impact on the duration it takes for the order to be delivered. This suggests that the online marketplaceâ€™s delivery operations are not strongly dependent on the weight of the products being delivered.\n \n\n\nTop sellers based on delivery time\n\n\nShow the Code\ntemp &lt;- orders |&gt;\n  left_join(select(order_items, order_id, seller_id), by = \"order_id\") |&gt;\n  left_join(sellers, by = \"seller_id\") |&gt;\n  select(order_purchase_timestamp, \n         order_delivered_customer_date, \n         seller_id, seller_city, seller_state) |&gt;\n  mutate(\n    time_diff = as.numeric(order_delivered_customer_date - order_purchase_timestamp)\n  )\n\n\n\n\nShow the Code\ntemp |&gt;\n  select(seller_id, time_diff) |&gt;\n  filter(!is.na(time_diff)) |&gt;\n  group_by(seller_id) |&gt;\n  summarise(n_order = n(),\n            avg = mean(time_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = duration(avg, \"hours\") |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = as.numeric(value),\n         period = factor(period, levels = period_levels),\n         seller_id = str_trunc(seller_id, width = 10, \"right\")) |&gt;\n  filter(period == \"days\", n_order &gt; 1) |&gt;\n  arrange(value) |&gt;\n  head(10) |&gt;\n  \n  gt() |&gt;\n  cols_label(seller_id = md(\"Seller **ID**\"),\n             n_order = md(\"*No.* of Order\"),\n             avg = \"Average\",\n             value = \"Days\") |&gt;\n  cols_hide(period) |&gt;\n  tab_header(\"Top Sellers Based on Average Order Duration\") |&gt;\n  fmt_number(avg)\n\n\n\n\n\n\n  \n    \n      Top Sellers Based on Average Order Duration\n    \n    \n    \n      Seller ID\n      No. of Order\n      Average\n      Days\n    \n  \n  \n    96f7c79...\n3\n46.71\n1.95\n    751e274...\n2\n47.94\n2.00\n    99a25c3...\n6\n53.54\n2.23\n    26562f2...\n2\n54.13\n2.26\n    734def0...\n2\n54.92\n2.29\n    0af9776...\n2\n60.97\n2.54\n    8def3db...\n2\n63.82\n2.66\n    aa1eb17...\n3\n68.09\n2.84\n    53b0300...\n7\n70.11\n2.92\n    99cd942...\n2\n72.31\n3.01\n  \n  \n  \n\n\n\n\nBy analyzing the average order duration of the top sellers on the platform, it becomes evident that these sellers consistently require more than 2 days, on average, to fulfill customer orders.\n\n\nSeller State:\n\n\nShow the Code\ntemp |&gt; \n  left_join(bz_state_name, by = c(\"seller_state\" = \"code\")) |&gt; \n  filter(!is.na(time_diff)) |&gt;\n  group_by(name) |&gt;\n  summarise(n_order = n(),\n            avg = mean(time_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = duration(avg, \"hours\") |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = as.numeric(value),\n         period = factor(period, levels = period_levels)) |&gt;\n  arrange(value)  |&gt; \n  \n  ggplot(aes(x = value, y = fct_reorder(name, value) |&gt; fct_rev())) +\n  geom_col(fill = colr$bar) +\n  labs(x = \"Duration Weeks\", \n       y = NULL, \n       title = \"Average Order Duration by Sellers State\") +\n  custom_theme()\n\n\n\n\n\nAmong the analyzed states, sellers from Rio Grande do Norte have the lowest average order duration of approximately 1.16 weeks. On the other hand, sellers in RondÃ´nia have the highest average order duration, with an average of 3 weeks.\n\n\nSeller city:\n\n\nShow the Code\ntemp |&gt;\n  filter(!is.na(time_diff)) |&gt;\n  group_by(seller_city) |&gt;\n  summarise(n_order = n(),\n            avg = mean(time_diff, na.rm = TRUE)) |&gt;\n  mutate(duration = duration(avg, \"hours\") |&gt; clean_duration()) |&gt;\n  separate(col = duration, into = c(\"value\", \"period\"), sep = \" \") |&gt;\n  mutate(value = as.numeric(value),\n         period = factor(period, levels = period_levels),\n         seller_city = str_to_title(seller_city)) |&gt;\n  arrange(value) |&gt;\n  head(10) |&gt;\n  \n  gt() |&gt;\n  cols_label(seller_city = \"City\",\n             n_order = md(\"*No.* Order\"),\n             avg = \"Average\",\n             value = \"Week\") |&gt;\n  cols_hide(period) |&gt;\n  tab_header(\"Top Cities with the Lowest Average Order Duration\") |&gt;\n  fmt_number(avg) \n\n\n\n\n\n\n  \n    \n      Top Cities with the Lowest Average Order Duration\n    \n    \n    \n      City\n      No. Order\n      Average\n      Week\n    \n  \n  \n    Auriflama\n9\n170.92\n1.02\n    Cordilheira Alta\n1\n171.32\n1.02\n    Rio Claro\n83\n170.91\n1.02\n    Braco Do Norte\n12\n172.95\n1.03\n    Leme\n4\n172.47\n1.03\n    Santa Barbara D Oeste\n8\n172.51\n1.03\n    Concordia\n9\n174.52\n1.04\n    Mandaguacu\n19\n174.83\n1.04\n    Natal\n15\n174.50\n1.04\n    Auriflama/Sp\n6\n179.67\n1.07\n  \n  \n  \n\n\n\n\nWhen examining the analysis by cities, it is observed that Auriflama, Cordilheira Alta, and Rio Claro have the lowest average order duration, each taking just 1.02 weeks. These cities stand out for their efficient order processing and delivery times. Sellers in these cities likely have optimized logistics, effective coordination with carriers, or streamlined operations, allowing them to fulfill orders promptly.\nThese findings emphasize regional variations in delivery efficiency and present an opportunity for the platform and sellers to share best practices, address challenges, and work towards improving order fulfillment times across different states and cities in Brazil."
  },
  {
    "objectID": "posts/Delivery-analysis/index.html#summary",
    "href": "posts/Delivery-analysis/index.html#summary",
    "title": "Product Delivery Analysis",
    "section": "Summary",
    "text": "Summary\nThe platform delivery analysis report provides valuable insights into various aspects of the delivery process. The report covers order duration, order status, order approval time, delivery to carriers, and delivery to customers. Here are the key highlights:\nOrder Duration:\nThe analysis reveals that the duration of order delivery to customers varied across different months. There was a positive decline in delivery duration over time, with the lowest median duration observed in August. However, a significant proportion of customers received their orders after a week, indicating room for improvement in reducing delivery times.\nOrder Status:\nThe majority of orders were successfully delivered to customers, reflecting a high success rate. However, a small percentage of orders were on transit or canceled, underscoring the importance of effective order tracking and proactive communication with customers regarding order status.\nOrder Approval Time:\nThe report indicates that most orders were approved within minutes of order initiation, while a small fraction took up to a week for approval. This highlights the need for efficient order approval processes to ensure timely order processing and enhance customer satisfaction.\nDelivery to Carriers:\nThe analysis shows that a significant number of orders were delivered to carriers in days, followed by hours and minutes. This suggests the need for streamlined coordination with carriers to ensure smooth and efficient handover of orders for delivery.\n\nBased on the analysis of the platformâ€™s delivery process, here are some recommendations to enhance the overall delivery experience for customers:\nStreamline Order Processing: Efforts should be made to further expedite the order approval process, aiming for faster approval times, particularly for orders that currently experience delays of up to a week. This can help reduce the overall order duration and improve customer satisfaction.\nEnhance Carrier Coordination: Strengthen collaboration with carriers to optimize the delivery process. This includes improving communication channels, ensuring accurate tracking information, and exploring ways to reduce transit times and increase efficiency in order handover."
  },
  {
    "objectID": "posts/reorder-point/index.html",
    "href": "posts/reorder-point/index.html",
    "title": "Product Reorder Point Calculation In Python",
    "section": "",
    "text": "Numerous businesses have experienced the predicament of being unable to fulfill customer demand due to inadequate order quantities and incorrect estimations of the required ordering intervals. As a result, they have experienced a loss in sales. This highlights the critical importance of accurately calculating the reorder point for achieving business success. Hence, it becomes essential to define the reorder point.\nA reorder point is a predetermined inventory level at which a new order is placed to replenish stock before it runs out completely. By accurately determining the reorder point, businesses can ensure uninterrupted product availability, minimize the risk of stockouts, and prevent loss of sales. Using python, we will perform the computation of the reorder point for a retail company."
  },
  {
    "objectID": "posts/reorder-point/index.html#extrating-important-dates",
    "href": "posts/reorder-point/index.html#extrating-important-dates",
    "title": "Reorder Point Calculation In Python",
    "section": "Extrating important dates",
    "text": "Extrating important dates\n\npast_orders = past_orders.assign(\n1    day_week = past_orders[\"date\"].dt.day_of_week,\n2    week = past_orders[\"date\"].dt.isocalendar().week,\n3    month = past_orders[\"date\"].dt.month_name(),\n4    year = past_orders[\"date\"].dt.year\n)\n\n\n1\n\nTake penguins and then\n\n2\n\nadd new columns.\n\n3\n\nadd new bill.\n\n4\n\nadd new ratio."
  },
  {
    "objectID": "posts/reorder-point/index.html#extrating-important-dates-periods",
    "href": "posts/reorder-point/index.html#extrating-important-dates-periods",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Extrating important dates periods",
    "text": "Extrating important dates periods\n\npast_orders = past_orders.assign(\n    week = past_orders[\"date\"].dt.isocalendar().week,  # (1)\n    year = past_orders[\"date\"].dt.year                 # (2)\n)\n\n\nThe week number within a year.\nThe order year."
  },
  {
    "objectID": "posts/reorder-point/index.html#weekly-demand",
    "href": "posts/reorder-point/index.html#weekly-demand",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Weekly Demand",
    "text": "Weekly Demand\nWe will examine the average and cummulative values of the weekly demand for all SKUs. This exploration will provide insights into both the typical demand level and the overall magnitude of the weekly demand throughout the year 2019.\n\nall_sku_summary = (\n    past_orders\n        .groupby([\"year\", \"week\"])                         # (1)\n        .agg(\n            avg_demand   = (\"order_quantity\", \"mean\"),\n            total_demand = (\"order_quantity\", \"sum\")\n        )\n        .reset_index()\n)\n\n\nThe past order will be grouped based on the order year and order week number.\n\n\n\nall_sku_summary_plot\ndef all_sku_summary_plot(df: pd.DataFrame, var: str):\n    \n    plt_title = \"Average\" if var == \"avg_demand\" else \"Total\"\n\n    return (\n\n        g.ggplot(df.query(\"year == 2019\"), g.aes(x=\"week\", y=var)) +\n        g.geom_line(color=\"#4F2F4F\") +\n        g.labs(x=\"Weeks\", \n               y=\"Demand\", \n               title=f\"{plt_title} Weekly Demand for All SKUs in 2019\") +\n        g.scale_x_continuous(breaks=range(0,53,5)) +\n        g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n        g.theme_minimal() +\n        g.theme(\n            figure_size=(10, 4),\n            plot_title=g.element_text(color=\"#555555\", ha=\"left\"),\n            axis_title=g.element_text(color=\"#666666\"),\n            plot_background=g.element_rect(fill=\"#FFFFFF\", color=\"#FFFFFF\")\n        )\n\n    )\n\n\n\nAverage DemandTotal Demand\n\n\n\nall_sku_summary_plot(all_sku_summary, \"avg_demand\")\n\n\n\n\n\n\n\nall_sku_summary_plot(all_sku_summary, \"total_demand\")\n\n\n\n\n\n\n\nThe analysis of the average weekly demand reveals noticable spikes in demand during specific weeks such as the 18th, 22nd, and 29th weeks, where the demand exceeded 600 units. Additionally, after the 34th week, the demand consistently remained above 300 units, unlike the first 15 weeks.\nFor the total total weekly demand, we observe that demand was relatively lower for the initial 32 weeks. However, a significant surge occured from the 34th week untill the end of the year, with a demand increase of over 200,000 units. This indicate a higher volume of orders required towards the end of the year compared to the beginning."
  },
  {
    "objectID": "posts/reorder-point/index.html#lead-time",
    "href": "posts/reorder-point/index.html#lead-time",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Lead Time",
    "text": "Lead Time\nBefore estimating the safety stock for each SKU, letâ€™s examine the average and maximum lead time required for product replenishment.\n\n\nlead_time_plot\ndef lead_time_plot(df: pd.DataFrame, var: str):\n\n    plt_title = \"Average\" if var == \"avg_lead_time_days\" else \"Maximum\"\n\n    return (\n\n        g.ggplot(df, g.aes(x=1, y=var)) +\n        g.geom_boxplot(color=\"#4F2F4F\", fill = \"#EEEEEE\") +\n        g.labs(x=None, \n               y=\"Lead Time (Days)\", \n               title=f\"Distribution of {plt_title} Lead Time\") +\n        g.theme_minimal() +\n        g.theme(\n            figure_size=(6, 4),\n            plot_title=g.element_text(color=\"#555555\", ha = \"left\"),\n            axis_title=g.element_text(color=\"#666666\"),\n            axis_text_x=g.element_blank(),\n            plot_background=g.element_rect(fill=\"#FFFFFF\", color=\"#FFFFFF\")\n        )\n        \n    )\n\n\n\nAverage Lead TimeMaximum Lead Time\n\n\n\nlead_time_plot(stock, \"avg_lead_time_days\")\n\n\n\n\n\n\n\nlead_time_plot(stock, \"max_lead_time_days\")\n\n\n\n\n\n\n\nWhen analyzing the average lead time for all SKUs, it is observed that 75% of the SKUs had lead times below 60 days. However, there are several other SKUs with an average lead time of 120 days and they are considered outliers.\nIn terms of the maximum lead time distribution, the majority (75%) of SKUs had lead time below 92 days. Nevertheless, there are some outliers with lead times exceeding 160 days. This indicate that the replenishment process can extend to durations of 160 days or even longer. Having such lengthy lead times posses significant challenges and difficulties."
  },
  {
    "objectID": "posts/reorder-point/index.html#average-maximum-demand-for-each-sku",
    "href": "posts/reorder-point/index.html#average-maximum-demand-for-each-sku",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Average & Maximum demand for each SKU",
    "text": "Average & Maximum demand for each SKU\n\ndemand = (\n    past_orders\n        .groupby([\"year\", \"week\", \"sku\"])\n        .agg(total_demand = (\"order_quantity\", \"sum\")) # (1)\n        .reset_index()\n        .groupby([\"sku\"])\n        .agg(\n            avg_demand = (\"total_demand\", \"mean\"),     # (2)\n            max_demand = (\"total_demand\", \"max\")       # (3)\n        )\n        .reset_index()\n)\n\n\nThe total demand per week for each SKUs.\nThe average quantity demand.\nThe maximum (peak) quantity demand.\n\n\ndemand.head()\n\n\n\n\n\n\n\n\nsku\navg_demand\nmax_demand\n\n\n\n\n0\n1009AA\n1252.144928\n7004.0\n\n\n1\n1077CA\n6951.169231\n34554.0\n\n\n2\n1083AA\n6552.644068\n24126.0\n\n\n3\n1116CA\n8518.940000\n36617.0\n\n\n4\n1126CA\n8158.431373\n35072.0\n\n\n\n\n\n\n\n\n\nsku_demand_plot\ndef sku_demand_plot(df: pd.DataFrame, var: str):\n\n    plt_title = \"Average\" if var == \"avg_demand\" else \"Maximum\"\n\n    return (\n\n        g.ggplot(df, g.aes(x=var)) +\n        g.geom_histogram(bins=50, fill=\"#4F2F4F\") +\n        g.labs(x=\"Demand\", y=\"Count\", title=f\"{plt_title} Demand for all SKUs\") +\n        g.scale_x_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l]) +\n        g.theme_minimal() +\n        g.theme(\n            figure_size=(8, 4),\n            plot_title=g.element_text(color=\"#555555\", ha=\"left\"),\n            axis_title=g.element_text(color=\"#666666\"),\n            plot_background=g.element_rect(fill=\"#FFFFFF\", color=\"#FFFFFF\")\n        )\n\n    )\n\n\n\n\nsku_demand_table\ndef sku_demand_table(df: pd.DataFrame, var: str):\n\n    return (\n    df[var]\n        .describe()\n        .reset_index()\n        .rename(columns={\"index\": \"stats\"})\n        .query(\"stats != 'count'\")\n        .style.format(formatter='{:,.2f}', subset=var, precision=1)\n)\n\n\n\nAverage DemandMaximum Demand\n\n\n\nsku_demand_plot(demand, \"avg_demand\")\n\n\n\n\n\nsku_demand_table(demand, \"avg_demand\")\n\n\n\n\n\n\nÂ \nstats\navg_demand\n\n\n\n\n1\nmean\n1,137.46\n\n\n2\nstd\n5,244.68\n\n\n3\nmin\n1.11\n\n\n4\n25%\n10.68\n\n\n5\n50%\n22.86\n\n\n6\n75%\n249.00\n\n\n7\nmax\n64,868.21\n\n\n\n\n\n\n\n\nsku_demand_plot(demand, \"max_demand\")\n\n\n\n\n\nsku_demand_table(demand, \"max_demand\")\n\n\n\n\n\n\nÂ \nstats\nmax_demand\n\n\n\n\n1\nmean\n5,417.22\n\n\n2\nstd\n17,286.52\n\n\n3\nmin\n2.00\n\n\n4\n25%\n47.17\n\n\n5\n50%\n206.50\n\n\n6\n75%\n1,706.50\n\n\n7\nmax\n136,634.40"
  },
  {
    "objectID": "posts/reorder-point/index.html#safety-stock",
    "href": "posts/reorder-point/index.html#safety-stock",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Safety Stock",
    "text": "Safety Stock\nSafety stock, also known as buffer stock, refers to the additional inventory held beyond the expected demand during the lead time. it acts as a cushion to account for uncertainties in demand, supply chain disruptions, or unexpected variations in lead time.\nThe formula for calculating safety stock using the maximum and average weekly demand is as follows:\n\nSafety stock = (max_unit_week x max_lead_time_replenish ) â€” (avg_weekly_demand x avg_lead_time_weeks)\n\nWhere:\nmax_unit_week = Maximum number of units demand in a week.\nmax_lead_time_replenish = Maximum lead time for stock replenishment.\navg_weekly_demand = Average weekly usage.\navg_lead_time_weeks = Average lead time in a week.\n\ninventory = (\n    demand\n        .merge(stock, how=\"left\", on=\"sku\")                                                # (1)\n        .assign(\n            avg_lead_time_weeks = lambda _: _[\"avg_lead_time_days\"] / 7,                   # (2)\n            max_lead_time_weeks = lambda _: _[\"max_lead_time_days\"] / 7,                   # (3)\n            max_stock_depletion = lambda _: _[\"max_demand\"] * _[\"max_lead_time_weeks\"],    # (4)\n            avg_safety_stock = lambda _: _[\"avg_demand\"] * _[\"avg_lead_time_weeks\"],       # (5)\n            safety_stock  = lambda _: _[\"max_stock_depletion\"] - _[\"avg_safety_stock\"]   \n        )\n)\n\n\nJoin the demand table and stock table using the sku variable.\nThe average weekly replenishment lead time.\nThe maximum weekly replenishment lead time.\nMaximum stock depletion during lead time.\nAverage safety stock for lead time.\n\n\ncol_list = [\n    \"sku\", \"current_stock_qty\", \"unit_price\", \n    \"max_demand\", \"avg_demand\",\n    \"avg_lead_time_weeks\", \"max_lead_time_weeks\",\n    \"max_stock_depletion\", \"avg_safety_stock\", \"safety_stock\"\n]\n\ninventory = inventory[col_list]\n\ninventory[[\n    \"sku\", \"max_stock_depletion\", \"avg_safety_stock\", \"safety_stock\"\n]].head()\n\n\n\n\n\n\n\n\nsku\nmax_stock_depletion\navg_safety_stock\nsafety_stock\n\n\n\n\n0\n1009AA\n48027.428571\n5366.335404\n42661.093168\n\n\n1\n1077CA\n345540.000000\n44686.087912\n300853.912088\n\n\n2\n1083AA\n234366.857143\n42124.140436\n192242.716707\n\n\n3\n1116CA\n732340.000000\n109529.228571\n622810.771429\n\n\n4\n1126CA\n310637.714286\n52447.058824\n258190.655462\n\n\n\n\n\n\n\n\n\n\ncurrent_vs_safety table\ncurrent_vs_safety = (\n    inventory[[\"current_stock_qty\", \"safety_stock\"]]\n        .assign(\n            fill_color = lambda _: np.where(\n                _[\"safety_stock\"] &gt; 1000000, \"red\",\n                np.where(\n                     _[\"current_stock_qty\"] &gt; 200000, \"red\", \"purple\"\n                )\n            )\n        )\n)\n\n\n\n\nRelationship plot Code\n(\n    g.ggplot(current_vs_safety, g.aes(x=\"current_stock_qty\", y=\"safety_stock\")) +\n    g.geom_point(g.aes(color = \"fill_color\"), show_legend=False) +\n    g.scale_y_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l])  +\n    g.scale_x_continuous(labels = lambda l: [\"{:,.0f}\".format(v) for v in l])  +\n    g.scale_color_manual(values = {\"red\": \"#CD0000\", \"purple\" : \"#4F2F4F\"}) +\n    g.labs(x=\"Stock Quantity (Kg)\", \n           y=\"Safety Stock\", \n           title=\"Relationship between Current Stock Qty. & Safety Stock\") +\n    g.theme_minimal() +\n    g.theme(\n        figure_size=(7, 4),\n         plot_title=g.element_text(color=\"#555555\", ha = \"left\", size=11),\n         axis_title=g.element_text(color=\"#666666\"),\n         plot_background=g.element_rect(fill=\"#FFFFFF\", color=\"#FFFFFF\")\n    )\n)\n\n\n\n\n\nThe plot above reveals the expected positive relationship between the current stock quantity and safety stock. However, there are a few notable outliers in this relationship. Some SKUs exhibit a high current stock quantity of over 300,000 units but have a relatively lower safety stock level of less tha 500,000 units. Conversely, certain SKUs have a safety stock of over 1.5million Kg while having a lower current stock quantity of less than 100,000 Kgs"
  },
  {
    "objectID": "posts/reorder-point/index.html#reorder-point",
    "href": "posts/reorder-point/index.html#reorder-point",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Reorder Point",
    "text": "Reorder Point\nReorder point is determined based on factors such as lead time, demand variability, and desired service level. It is calculated by considering the average weekly demand, lead time, and safety stock.\nWhen the inventory level reached or falls below the reorder point, it triggers the placement of a new order to replenish the stock.\nThe formula for calculating reorder point is as follows:\n\nReorder point = (avg_units_used x avg_week_lead_time) + safety_stock_units\n\nWhere: avg_units_used = Average demand of units weekly.\navg_week_lead_time = Average of week lead time.\n\ninventory = (\n    inventory\n        .assign(\n            qty_unit_sold = (inventory[\"avg_demand\"] * inventory[\"avg_lead_time_weeks\"]), # (1)\n            reorder_point   = lambda _: _[\"safety_stock\"] + _[\"qty_unit_sold\"],           # (2)\n            need_to_reorder = lambda _: np.where(                                         # (3)\n                _[\"reorder_point\"] &gt; _[\"current_stock_qty\"], \"Yes\", \"No\" \n            )\n        )\n)\n\n\nThe total quantity of units that would be sold during the lead time.\nProduct reorder point.\nWhether there is a need to replenish stock or not.\n\nThe safety stock here acts as buffer or cushion to account for uncertainties, variations in demand, and other factors that may affect the availability of stock during the lead time. In other words it ensures that there is enough stock on hand to avoid stockout and meet customer demand even in unexpected situations.\n\ninventory[[\n    \"sku\", \"safety_stock\", \"qty_unit_sold\", \n    \"reorder_point\", \"need_to_reorder\"\n]].head()\n\n\n\n\n\n\n\n\nsku\nsafety_stock\nqty_unit_sold\nreorder_point\nneed_to_reorder\n\n\n\n\n0\n1009AA\n42661.093168\n5366.335404\n48027.428571\nYes\n\n\n1\n1077CA\n300853.912088\n44686.087912\n345540.000000\nYes\n\n\n2\n1083AA\n192242.716707\n42124.140436\n234366.857143\nYes\n\n\n3\n1116CA\n622810.771429\n109529.228571\n732340.000000\nYes\n\n\n4\n1126CA\n258190.655462\n52447.058824\n310637.714286\nYes\n\n\n\n\n\n\n\n\n\nneed_reorder = (\n    inventory[\"need_to_reorder\"]\n        .value_counts()\n        .reset_index()\n        .rename(columns= {\"index\": \"need_to_reorder\", \"need_to_reorder\": \"count\"})\n        .assign(\n            percentage = lambda _: round((_[\"count\"] / _[\"count\"].sum()) * 100, 2)\n        )\n)\n\n\n\nNeed to reorder plot code\n(\n    g.ggplot(need_reorder, g.aes(x=\"need_to_reorder\", y=\"count\")) +\n    g.geom_col(fill=\"#4F2F4F\") +\n    g.geom_text(\n        g.aes(label=\"count\", y=4), \n        position=g.position_dodge(width=.9), \n        nudge_x=-0.25,\n        size=27, \n        color=\"#BA55D3\",\n        va=\"bottom\"\n    ) +\n    g.geom_text(\n        g.aes(label = \"percentage\", y=7), \n        position=g.position_dodge(width=.9), \n        nudge_x=0.27, \n        size=15, \n        va=\"bottom\",\n        color=\"#E066FF\",\n        format_string=\"({}%)\"\n    ) +\n    g.labs(x=\"Reorder?\", y=None, title=\"Number of SKUs in each Category\") +\n    g.theme_minimal() +\n    g.theme(\n        figure_size=(7, 3),\n        plot_title=g.element_text(color=\"#555555\", ha = \"left\"),\n        axis_title_x=g.element_text(color=\"#666666\"),\n        axis_text_y=g.element_blank(),\n        plot_background=g.element_rect(fill=\"#FFFFFF\", color=\"#FFFFFF\")\n    )\n)\n\n\n\n\n\nBased on the bar chart, it is evident that approximately 281 SKUs, which accounts for 96.9% of the total, require replenishment. Conversely, only 9 SKUs (3.1%) do not require replenishment. This indicate that majority of SKUs in the inventory, almost all except for a small minority, are in need of restocking."
  },
  {
    "objectID": "posts/reorder-point/index.html#stock-status",
    "href": "posts/reorder-point/index.html#stock-status",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Stock Status",
    "text": "Stock Status\nTo ensure precise categorization of each SKU, it is necessary to extract the current stock status.\n\ninventory = (\n    inventory.\n        assign(\n            stock_status = lambda _: np.where(                                        # (1)                       \n                _[\"current_stock_qty\"] == 0, \"Out of Stock\", np.where(                # (2)\n                    _[\"need_to_reorder\"] == \"Yes\", \"Below Reorder Point\", \"In Stock\"  \n                )                                                                     \n            )\n        )\n)\n\n\nCategory of SKUs that a currently not available.\nCategory of SKUs below the reorder point and SKUs currently in stock.\n\n\ninventory[\n    [\"sku\", \"safety_stock\", \"reorder_point\", \"need_to_reorder\", \"stock_status\"]\n].head()\n\n\n\n\n\n\n\n\nsku\nsafety_stock\nreorder_point\nneed_to_reorder\nstock_status\n\n\n\n\n0\n1009AA\n42661.093168\n48027.428571\nYes\nBelow Reorder Point\n\n\n1\n1077CA\n300853.912088\n345540.000000\nYes\nBelow Reorder Point\n\n\n2\n1083AA\n192242.716707\n234366.857143\nYes\nBelow Reorder Point\n\n\n3\n1116CA\n622810.771429\n732340.000000\nYes\nBelow Reorder Point\n\n\n4\n1126CA\n258190.655462\n310637.714286\nYes\nBelow Reorder Point\n\n\n\n\n\n\n\n\n\nstock_status = (\n    inventory[\"stock_status\"]\n        .value_counts()\n        .reset_index()\n        .rename(columns= {\"index\": \"stock_status\", \"stock_status\": \"count\"})\n        .assign(\n            percentage = lambda _: round((_[\"count\"] / _[\"count\"].sum()) * 100, 2)\n        )\n)\n\n\n\nStock status plot code\n(\n    g.ggplot(stock_status, g.aes(x=\"stock_status\", y=\"count\")) +\n    g.geom_col(fill=\"#4F2F4F\") +\n    g.geom_text(\n        g.aes(label=\"count\", y=4), \n        position=g.position_dodge(width=.9), \n        nudge_x=-0.25,\n        size=27, \n        color=\"#BA55D3\",\n        va=\"bottom\"\n    ) +\n    g.geom_text(\n        g.aes(label = \"percentage\", y=7), \n        position=g.position_dodge(width=.9), \n        nudge_x=0.25, \n        size=14, \n        va=\"bottom\",\n        color=\"#E066FF\",\n        format_string=\"({}%)\"\n    ) +\n    g.labs(x=None, y=None, title = \"Number of SKUs by Stock Status\") +\n    g.theme_minimal() +\n    g.theme(\n        figure_size=(9, 4), \n        plot_title=g.element_text(color=\"#555555\", ha = \"left\"),\n        axis_title_x=g.element_text(color=\"#666666\"),\n        axis_text_y=g.element_blank(),\n        plot_background=g.element_rect(fill=\"#FFFFFF\", color=\"#FFFFFF\")\n    )\n)\n\n\n\n\n\nThe analysis reveals that a significant majority of SKUs (approximately 96.55%) were currently below the reorder point, indicating a potential risk of stockouts. Only 3.1% of SKUs were found to be in stock and remarkably, only one SKU was out of stock.\nThis finding highlights the need for imporvement in inventory management to ensure a higher percentage of SKUs are maintained above the reorder point and readily available for customers."
  },
  {
    "objectID": "posts/reorder-point/index.html#benefits-of-accurate-reorder-points",
    "href": "posts/reorder-point/index.html#benefits-of-accurate-reorder-points",
    "title": "Product Reorder Point Calculation In Python",
    "section": "Benefits Of Accurate Reorder Points",
    "text": "Benefits Of Accurate Reorder Points\n\nIt ensures optimal inventory level by replenishing stock at the right time.\nImproved customer satisfaction by ensuring that products are consistently available to meet customer demand.Therefore customer can rely on the business to fulfill orders promptly and consistently.\nAccurate reorder points can help businesses avoid excess inventory, thereby reducing storage costs, minimize the risk of obsolescence, and optimize their working capital.\nEfficient supply chain management can be achieved by streamlining the flow of goods, optimize procument processes, and maintain strong relationship with suppliers. This can imporve lead times and better coordination throughout the supply chain.\nIt enhance business agility by enabling quick adjustments to inventory levels in response to changes in market demand, customer preferences, or unexpected events."
  },
  {
    "objectID": "posts/normalization/index.html",
    "href": "posts/normalization/index.html",
    "title": "Demystifying Database Normalization: Moving Single CSV Files into a Structured Database",
    "section": "",
    "text": "In todayâ€™s data-driven world, businesses often find themselves grappling with large amount of data in order to derive meaningful insights. As this data volume increases, the performance of their existing small-scale data storage solutions, such as storing data in a single CSV file, starts to deteriorate. Consequently, businesses face the risk of not keeping up with the latest market trends and customer purchase patterns. To address this challenge, migrating the data into a database management system, either on-site or in the cloud, provides an excellent solution. In this article, we will explore the process of migrating a single CSV file that contains records tracking customer orders. Additionally, we will delve into the concept of database normalization and outline the steps involved in moving the data.\n\n\n\nERD\n\n\n\nUnderstanding database normalization.\nBefore delving into the details of data migration and normalization of our database, it is crucial to understand the meaning of normalization. So what is database normalization, Database normalization is process of organizing and structuring data to minimize redundancy, improve data integrity, and optimize database performance. Simply put, it involves breaking down a database into multiple related tables, each serving a specific purpose.\nThere are several important reasons for optimizing a database, Such as to:\n\nImproved data integrity by reducing redundancy and inconsistency.\nEfficiently utilize storage by eliminating data duplications.\nEnhance performance and ensure quick response times.\nFacilitate flexibility and scalability for future modifications and expansions.\nAchieve complience and security by implementing a normalized structure that simplifies access control.\n\nWhile i wonâ€™t delve too deeply into these concepts to stay focused on the task at hand, it is crucial to have a good understanding of these principles for effective database management.\n\n\nCreating the Database\nBefore migrating your data from a single csv file into a database, it is important to plan the database structure and consider how you will handle data redundancy to ensure data consistency and integrity. Additionaly, there are other important principles to look out for, such as establishing proper relationships between the tables, defining primary and foreign keys, and implementing approprate data validation rules.\nWe will be using the postgreSQL relational database for the migration process and data storage. However, the process will be similar for other relational database. To begin, we will create a database called order_flow where we will be working on as we progress through this article.\n\nCREATE DATABASE order_flow;\n\nThe above query will create a database and establish a new session after you run it. Now that we have a functional database, the next step is to create an initial table that will serve as the container for the data currently residing in the CSV file.\n\nCREATE TABLE denormalized_table (\n    table_id SERIAL PRIMARY KEY,\n    product_name VARCHAR(12),\n    supplier_name VARCHAR(35),\n    customer_id INTEGER,\n    order_date TIMESTAMP,\n    ship_date TIMESTAMP,\n    quantity SMALLINT,\n    price DECIMAL(6, 3),\n    customer_first_name VARCHAR(20),\n    customer_last_name VARCHAR(20)\n);\n\nAfter running the above query, you are likely to see a message that says Query returned successfully This message simply indicates that our table has been created successfully. To further confirm the tableâ€™s present status, you can run the query below, which will return the column names and an empty row. The empty row is expected since we havenâ€™t inserted any records yet.\n\nSELECT * FROM denormalized_table;\n\n\n\n\n\n\n\n\n\n\ntable_id\nproduct_name\nsupplier_name\ncustomer_id\norder_date\nship_date\nquantity\nprice\ncustomer_first_name\ncustomer_last_name\n\n\n\n\n\n\n\n\n\n\nNow itâ€™s time to import the data from the CSV file. We will use the COPY command in PostgreSQL, which allows for importing CSV files. Additionally, we will inclued the CSV HEADER option at the end of the query, without the CSV HEADER option, PostgreSQL would treat the first row of the CSV file as data and attempt to insert it into the table, leading to incorrect data placement and potential data integrity issues. Therefore it is good practice to include the option when importing CSV files.\n\nCOPY denormalized_table (\n    table_id,\n    product_name,\n    supplier_name,\n    customer_id,\n    order_date,\n    ship_date,\n    quantity,\n    price,\n    customer_first_name,\n    customer_last_name\n)\nFROM 'data/orders.csv'\nDELIMITER ','\nCSV HEADER;\n\n\nSELECT * FROM denormalized_table LIMIT 5;\n\n\n\n\n\n\n\n\n\n\ntable_id\nproduct_name\nsupplier_name\ncustomer_id\norder_date\nship_date\nquantity\nprice\ncustomer_first_name\ncustomer_last_name\n\n\n\n\n0\n0\nProduct E\nAustin-Phillips\n79294\n2023-04-19 13:25:16.815733\n2023-04-26 13:25:16.815733\n2\n104.212\nChristine\nGregory\n\n\n1\n1\nProduct C\nTucker Ltd\n46269\n2022-12-12 13:25:16.815733\n2022-12-19 13:25:16.815733\n10\n109.078\nArthur\nThompson\n\n\n2\n2\nProduct E\nAustin-Phillips\n6836\n2023-02-20 13:25:16.815733\n2023-02-24 13:25:16.815733\n7\n110.403\nMary\nDiaz\n\n\n3\n3\nProduct C\nAdams Group\n45014\n2023-04-09 13:25:16.815733\n2023-04-16 13:25:16.815733\n5\n119.123\nFrancisco\nMiller\n\n\n4\n4\nProduct E\nTucker Ltd\n74464\n2023-07-06 13:25:16.815733\n2023-07-11 13:25:16.815733\n5\n109.081\nMiguel\nThomas\n\n\n\n\n\n\n\n\nThe duration of the data import process may vary depending on the size of your data. Once the data import is complete, you will receive a message similar to the previous one indicating that the query has returned successfully. If you encounter any issues while importing the data, it is advisable to verify whether the column data types in your table match the data type definitions in the CSV file. Addtionally, ensure that the column positions in both instances are aligned correctly.\nWe have completed the first part of the process by successfully migrating our data into PostgreSQL. In the next part we will begin creating the specific entities.\n \n\n\nDatabase Normalization\nWhen separating your data into separate tables, it is crucial to understand the concept of normal forms (1NF, 2NF, 3NF, etc.), as these guidelines will assist you in creating an optimized database. Hereâ€™s a brief summary of the first three normal forms:\n\nFirst Normal Form (1NF): Eliminate data redundancy and ensures atomic values by organizing data into separate columns within a table.\n\n\nSecond Normal Form (2NF): Builds upon the 1NF and addresses partial depencencies by ensuring that non-key attributes are fully dependent on the entire primary key.\n\n\nThird Normal Form (3NF): Further refines the normalization process by eliminating transitive dependencies, ensuring that non-key attributes are not dependent on other non-key attributes.\n\nBy adhering to the principles of normalization and applying the approprate normal forms, you can effectively structure your database.\nNow, we will start by creating a Product Table that will solely contain infomation about the product and specific attributes related to each listed product.\n\nCREATE TABLE product (\n    prod_id SERIAL PRIMARY KEY,\n    prod_name VARCHAR(12) NOT NULL\n);\n\nSELECT * FROM product;\n\n\n\n\n\n\n\n\n\n\nprod_id\nprod_name\n\n\n\n\n\n\n\n\n\n\nJust like the product table, we will create a Supplier Table that will store information about the suppliers. This table will allow us capture specific attributes related to each supplier.\n\nCREATE TABLE supplier(\n    sup_id SERIAL PRIMARY KEY,\n    sup_name VARCHAR(35) NOT NULL\n);\n\nSELECT * FROM product;\n\n\n\n\n\n\n\n\n\n\nsup_id\nsup_name\n\n\n\n\n\n\n\n\n\n\nThe Customer Table will store important attributes such as unique customer IDs, first names and last names. These attributes will allow us to uniquely identify and manage customer infomation within the database.\n\nCREATE TABLE customer (\n    cus_id INTEGER PRIMARY KEY,\n    cus_first_name VARCHAR(20),\n    cus_last_name VARCHAR(20)\n);\n\nSELECT * FROM customer;\n\n\n\n\n\n\n\n\n\n\ncus_id\ncus_first_name\ncus_last_name\n\n\n\n\n\n\n\n\n\n\nFinally, we have the Order Table, which will serve as our fact table. This table will include fields such as order ID, product ID, suppler ID, customer ID, order date, ship date, quantity ordered and unit price.\n\nCREATE TABLE orders(\n    ord_id SERIAL PRIMARY KEY,\n    ord_product_id SERIAL,\n    ord_supplier_id SERIAL,\n    ord_customer_id INTEGER,\n    ord_order_date TIMESTAMP,\n    ord_ship_date TIMESTAMP,\n    ord_quantity SMALLINT,\n    ord_price DECIMAL(6, 3),\n    CONSTRAINT fk_ord_product FOREIGN KEY(ord_product_id) REFERENCES product(prod_id),\n    CONSTRAINT fk_ord_supplier FOREIGN KEY(ord_supplier_id) REFERENCES supplier(sup_id),\n    CONSTRAINT fk_ord_customer FOREIGN KEY(ord_customer_id) REFERENCES customer(cus_id)\n);\n\nSELECT * FROM orders;\n\n\n\n\n\n\n\n\n\n\nord_id\nord_product_id\nord_supplier_id\nord_customer_id\nord_order_date\nord_ship_date\nord_quantity\nord_price\n\n\n\n\n\n\n\n\n\n\nSince the order table serves as our fact table, it will establish foreign keys to the tables mentioned earlier. These foreign keys establish the relationship between the order table and the product, supplier, and customer tables. By linking the fact table with the ralated dimension table through foreign keys, we can perform comprehensive analyses and generate meaningful insights by combining data from different tables.\n \n\n\nAdding data to the various database tables\nAfter successfully creating the various tables, the next step is to migrate the centralized data into their respective tables.\nFor the Product Table, the process is straightforward. From the denormalized_table, we only need the unique product name. PostgreSQL will automatically generate and populate the product ID, which serves as the primary key for the product table.\n\nINSERT INTO product(prod_name)\nSELECT DISTINCT product_name\nFROM denormalized_table\nORDER BY 1;\n\nSELECT * FROM product;\n\n\n\n\n\n\n\n\n\n\nprod_id\nprod_name\n\n\n\n\n0\n1\nProduct A\n\n\n1\n2\nProduct B\n\n\n2\n3\nProduct C\n\n\n3\n4\nProduct D\n\n\n4\n5\nProduct E\n\n\n\n\n\n\n\n\nWe will follow a similar process for the Supplier Table. From the denormalized_table, we will transfer the unique supplier name to the supplier table while the supplier ID will be automatically generated and populated.\n\nINSERT INTO supplier(sup_name)\nSELECT DISTINCT supplier_name\nFROM denormalized_table\nORDER BY 1;\n\nSELECT * FROM supplier;\n\n\n\n\n\n\n\n\n\n\nsup_id\nsup_name\n\n\n\n\n0\n1\nAdams Group\n\n\n1\n2\nAustin-Phillips\n\n\n2\n3\nLarsen, Ballard and Hudson\n\n\n3\n4\nPruitt LLC\n\n\n4\n5\nTucker Ltd\n\n\n\n\n\n\n\n\nFor the Customer Table, we will transfer both the customerâ€™s first name and last name from the denormalized_table into the respective columns of the customer table. Additionally, the unique customer ID will also be migrated into the customer table preserving the primary key association.\n\nINSERT INTO customer(\n    cus_id,\n    cus_first_name,\n    cus_last_name\n)\nSELECT DISTINCT customer_id, customer_first_name, customer_last_name\nFROM denormalized_table\nORDER BY 1;\n\nSELECT * FROM customer LIMIT 5;\n\n\n\n\n\n\n\n\n\n\ncus_id\ncus_first_name\ncus_last_name\n\n\n\n\n0\n1\nChristine\nFernandez\n\n\n1\n2\nEric\nJensen\n\n\n2\n3\nEric\nStanley\n\n\n3\n4\nRebecca\nGomez\n\n\n4\n5\nSarah\nHenderson\n\n\n\n\n\n\n\n\nFinally, when migrating data into the Orders Table, we will take a different approach. All the attributes for each order will be directly transferred into the orders table, except for the foreign keys representing the product and supplier. To assign the appropriate product and supplier IDs to each record, we will use a subquery. This subquery will extract the respective IDs from the product and supplier tables based on their names. By leveraging this approach, we can accurately establish the relatioships between orders, products and suppliers within the order table, ensuring data consistency and integrity.\n\nINSERT INTO orders(\n    ord_id,\n    ord_product_id,\n    ord_supplier_id,\n    ord_customer_id,\n    ord_order_date,\n    ord_ship_date,\n    ord_quantity,\n    ord_price\n)\nSELECT \n    table_id, \n    (SELECT prod_id FROM product WHERE prod_name = product_name) AS ord_product_id,\n    (SELECT sup_id FROM supplier WHERE sup_name = supplier_name) AS ord_supplier_id,\n    customer_id,\n    order_date,\n    ship_date,\n    quantity,\n    price\nFROM denormalized_table\nORDER BY 1;\n\nSELECT * FROM orders LIMIT 5;\n\n\n\n\n\n\n\n\n\n\nord_id\nord_product_id\nord_supplier_id\nord_customer_id\nord_order_date\nord_ship_date\nord_quantity\nord_price\n\n\n\n\n0\n0\n5\n2\n79294\n2023-04-19 13:25:16.815733\n2023-04-26 13:25:16.815733\n2\n104.212\n\n\n1\n1\n3\n5\n46269\n2022-12-12 13:25:16.815733\n2022-12-19 13:25:16.815733\n10\n109.078\n\n\n2\n2\n5\n2\n6836\n2023-02-20 13:25:16.815733\n2023-02-24 13:25:16.815733\n7\n110.403\n\n\n3\n3\n3\n1\n45014\n2023-04-09 13:25:16.815733\n2023-04-16 13:25:16.815733\n5\n119.123\n\n\n4\n4\n5\n5\n74464\n2023-07-06 13:25:16.815733\n2023-07-11 13:25:16.815733\n5\n109.081\n\n\n\n\n\n\n\n\nAnd thatâ€™s it! We have successfully migrated and normalised our database. Before proceeding to delete the denormalized_table, it is good practice to test the relatioships between these tables. while our database is relatively simple, it is still valuable to verify the relationships that exist among the tables.\nOrder and Product Relationship\n\nSELECT * \nFROM orders \nJOIN product ON ord_product_id = prod_id\nLIMIT 10;\n\n\n\n\n\n\n\n\n\n\nord_id\nord_product_id\nord_supplier_id\nord_customer_id\nord_order_date\nord_ship_date\nord_quantity\nord_price\nprod_id\nprod_name\n\n\n\n\n0\n0\n5\n2\n79294\n2023-04-19 13:25:16.815733\n2023-04-26 13:25:16.815733\n2\n104.212\n5\nProduct E\n\n\n1\n1\n3\n5\n46269\n2022-12-12 13:25:16.815733\n2022-12-19 13:25:16.815733\n10\n109.078\n3\nProduct C\n\n\n2\n2\n5\n2\n6836\n2023-02-20 13:25:16.815733\n2023-02-24 13:25:16.815733\n7\n110.403\n5\nProduct E\n\n\n3\n3\n3\n1\n45014\n2023-04-09 13:25:16.815733\n2023-04-16 13:25:16.815733\n5\n119.123\n3\nProduct C\n\n\n4\n4\n5\n5\n74464\n2023-07-06 13:25:16.815733\n2023-07-11 13:25:16.815733\n5\n109.081\n5\nProduct E\n\n\n5\n5\n4\n5\n91152\n2022-11-05 13:25:16.815733\n2022-11-10 13:25:16.815733\n7\n119.146\n4\nProduct D\n\n\n6\n6\n4\n5\n77229\n2023-05-06 13:25:16.815733\n2023-05-12 13:25:16.815733\n9\n122.462\n4\nProduct D\n\n\n7\n7\n5\n3\n19772\n2023-02-09 13:25:16.815733\n2023-02-10 13:25:16.815733\n5\n117.377\n5\nProduct E\n\n\n8\n8\n5\n5\n85348\n2023-03-06 13:25:16.815733\n2023-03-13 13:25:16.815733\n3\n117.703\n5\nProduct E\n\n\n9\n9\n2\n5\n67895\n2023-05-19 13:25:16.815733\n2023-05-20 13:25:16.815733\n2\n117.743\n2\nProduct B\n\n\n\n\n\n\n\n\nOrder and Supplier Relationship\n\nSELECT * \nFROM orders \nJOIN supplier ON ord_supplier_id = sup_id\nLIMIT 10;\n\n\n\n\n\n\n\n\n\n\nord_id\nord_product_id\nord_supplier_id\nord_customer_id\nord_order_date\nord_ship_date\nord_quantity\nord_price\nsup_id\nsup_name\n\n\n\n\n0\n0\n5\n2\n79294\n2023-04-19 13:25:16.815733\n2023-04-26 13:25:16.815733\n2\n104.212\n2\nAustin-Phillips\n\n\n1\n1\n3\n5\n46269\n2022-12-12 13:25:16.815733\n2022-12-19 13:25:16.815733\n10\n109.078\n5\nTucker Ltd\n\n\n2\n2\n5\n2\n6836\n2023-02-20 13:25:16.815733\n2023-02-24 13:25:16.815733\n7\n110.403\n2\nAustin-Phillips\n\n\n3\n3\n3\n1\n45014\n2023-04-09 13:25:16.815733\n2023-04-16 13:25:16.815733\n5\n119.123\n1\nAdams Group\n\n\n4\n4\n5\n5\n74464\n2023-07-06 13:25:16.815733\n2023-07-11 13:25:16.815733\n5\n109.081\n5\nTucker Ltd\n\n\n5\n5\n4\n5\n91152\n2022-11-05 13:25:16.815733\n2022-11-10 13:25:16.815733\n7\n119.146\n5\nTucker Ltd\n\n\n6\n6\n4\n5\n77229\n2023-05-06 13:25:16.815733\n2023-05-12 13:25:16.815733\n9\n122.462\n5\nTucker Ltd\n\n\n7\n7\n5\n3\n19772\n2023-02-09 13:25:16.815733\n2023-02-10 13:25:16.815733\n5\n117.377\n3\nLarsen, Ballard and Hudson\n\n\n8\n8\n5\n5\n85348\n2023-03-06 13:25:16.815733\n2023-03-13 13:25:16.815733\n3\n117.703\n5\nTucker Ltd\n\n\n9\n9\n2\n5\n67895\n2023-05-19 13:25:16.815733\n2023-05-20 13:25:16.815733\n2\n117.743\n5\nTucker Ltd\n\n\n\n\n\n\n\n\nOrder and Customer Reletionship\n\nSELECT * \nFROM orders\nJOIN customer ON ord_customer_id = cus_id\nLIMIT 10;\n\n\n\n\n\n\n\n\n\n\nord_id\nord_product_id\nord_supplier_id\nord_customer_id\nord_order_date\nord_ship_date\nord_quantity\nord_price\ncus_id\ncus_first_name\ncus_last_name\n\n\n\n\n0\n0\n5\n2\n79294\n2023-04-19 13:25:16.815733\n2023-04-26 13:25:16.815733\n2\n104.212\n79294\nChristine\nGregory\n\n\n1\n1\n3\n5\n46269\n2022-12-12 13:25:16.815733\n2022-12-19 13:25:16.815733\n10\n109.078\n46269\nArthur\nThompson\n\n\n2\n2\n5\n2\n6836\n2023-02-20 13:25:16.815733\n2023-02-24 13:25:16.815733\n7\n110.403\n6836\nMary\nDiaz\n\n\n3\n3\n3\n1\n45014\n2023-04-09 13:25:16.815733\n2023-04-16 13:25:16.815733\n5\n119.123\n45014\nFrancisco\nMiller\n\n\n4\n4\n5\n5\n74464\n2023-07-06 13:25:16.815733\n2023-07-11 13:25:16.815733\n5\n109.081\n74464\nMiguel\nThomas\n\n\n5\n5\n4\n5\n91152\n2022-11-05 13:25:16.815733\n2022-11-10 13:25:16.815733\n7\n119.146\n91152\nShannon\nVega\n\n\n6\n6\n4\n5\n77229\n2023-05-06 13:25:16.815733\n2023-05-12 13:25:16.815733\n9\n122.462\n77229\nJoshua\nMartinez\n\n\n7\n7\n5\n3\n19772\n2023-02-09 13:25:16.815733\n2023-02-10 13:25:16.815733\n5\n117.377\n19772\nPenny\nSchmidt\n\n\n8\n8\n5\n5\n85348\n2023-03-06 13:25:16.815733\n2023-03-13 13:25:16.815733\n3\n117.703\n85348\nAlan\nShelton\n\n\n9\n9\n2\n5\n67895\n2023-05-19 13:25:16.815733\n2023-05-20 13:25:16.815733\n2\n117.743\n67895\nJames\nJones\n\n\n\n\n\n\n\n The relationship for each entity worked as expected, so we can go ahead and drop the denormalized_table\n\nDROP TABLE denormalized_table;\n\n \nTo conclued the article, the process of migrating and normalizing our database has allowed us to achieve several significate benefits. By organizing the data into separate tables and applying the principles of normalization, we have minimized data redundancy, improved data integrity, and optimized database performance. Through this process, we have created a structured database that facilitate efficient data management, analysis, and retrieval.\nHowever, there are areas where further improvement can be made, such as considering ongoing maintenance and scalability. As the database grows and evolves over time, it may be necessary to revist the normalization process and make adjustments to accommodate new requirements."
  }
]